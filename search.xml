<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Decision Tree Learning]]></title>
    <url>%2Fblog%2F2018%2F04%2F18%2FDecision-Tree-Learning%2F</url>
    <content type="text"><![CDATA[Decision Tree Learning 決策樹學習 簡介 最受歡迎的歸納推理演算法( inductive inference algorithm )。 廣泛且實務的方法。 對於干擾值( Noise )相當敏感。 可用來學習如何以聯集( Disjunctive )表示限制集( Constraints )。 ( Concept Learning 以交集( Conjunctive )表示 ) 呈現的方式相當簡單。 樹狀結構( Tree Structure )、若則表示式( If-Then rules ) $Ex.$ Play Tennis範例訓練資料( Training Example ) 決策樹( Decision Tree ) 決策樹( Decision Tree )的介紹決策樹表示法( Decision Tree Representation ) 每個內節點 Internal node ( 包括根結點 Root node )代表對一個環境狀態( Attribute )檢驗。 而分支( Branch )出來的意義我們可以視為是該環境狀態( Attribute )的一種可能值( Attribute value )。 每個葉節點 Leaf node 給予一個適當的分類結果( Classification )。 我們將每個案例( Instances )分類到一個離散的類別( Categories )之中。 藉由決策樹由根結點至葉節點找到該類別。 決策樹引導的假說( Hypotheses ) 先 AND 再 OR ( 原文：Disjunctions (OR’s) of conjunctions (AND’s) )。 經由根結點往葉節點走可視為是一種對於該環境狀態限制的交集( Conjunction of constraints on attributes )。 而連上兄弟節點( Sibling )的兩個邊( Edges )可視為是一種對於該環境狀態限制的聯集( Separate branches are disjunctions )。 $Ex \; ( Cont. )$ (Outlook=Sunny and Humidity=Normal) or (Outlook=Overcast) or (Outlook=Rain and Wind=Weak) 注意！ 每個用來訓練的案例 ( Instances )都必須要以「因素-結果」( Attribute - value pairs )的方式給予訓練。 目標訓練函式 ( Target function )的值域是離散的值 ( Discrete value )。 這種方法最後呈現的假說 ( Hypotheses )有可能是一些環境狀態限制( Constraints on attributes )的聯集( Disjunctive )。 極有可能會被不乾淨的資料( Noise )擾亂了學習。 應用於： 醫療或是設備的診斷。 信用額度分析( 銀行 )。 決策樹的種類世界上有許多有特殊的決策樹演算法( decision-tree algorithms )，比較著名的有： ID3 (Iterative Dichotomiser 3) C4.5, C5.0 (successor of ID3) CART (Classification And Regression Tree) CHAID (CHi-squared Automatic Interaction Detector). MARS: extends decision trees to handle numerical data better. 注意 ID3 is the algorithm discussed in textbook.( 在書本中有更詳細的介紹 ) Simple, but representative. ( 簡單但representative? ) Source code publicly available. ( 程式碼是開放的 ) ID3演算法 概述：Top-down, greedy search through space of possible decision trees. ( 在所有可以出現的決策樹中用貪心法由上而下找到較佳的那棵樹。 ) ID3在建構決策樹過程中，以資訊獲利(Information Gain)為準則，並選擇最大的資訊獲利值作為分類屬性。這個算法是建立在奧卡姆剃刀的基礎上：越是小型的決策樹越優於大的決策樹。儘管如此，該算法也不是總是生成最小的樹形結構，而是一個啟發式算法。另外，C4.5算法是ID3的升級版。 Decision trees represent hypotheses, so this is a search through hypothesis space. ( 決策樹亦也代表是一種假說，所以這個演算法也可以說是在所有的假說中找到一個較佳的假說。 ) 那個演算法該如何起手呢？ 決定甚麼環境因素( Attribute )應該放在根結點( Root node )？ 接著由上而下( Top-down )的建構決策樹，對每個後繼的節點( Successive node )使出一樣的決策手段選出該節點應該置入何種環境因素( Attribute )。 注意！千萬不要由下往上參考之前選過的值，因為我們以貪心法則，所以目前的最佳解決不可能出現在之前選過的環境因素( Attribute )之中，或是受其干擾。 ( Never backtracks to reconsider earlier choices. ) 同上述，在每次的選擇之中，由於我們認知這種情況適用貪心法( Greedy Method )，所以我們每次環境因素( Attribute )的選擇都朝向我們最後最佳的假說靠近。 虛擬碼( Pseudo Code )12341. 使用屬性計算與之相關的樣本熵值2. 選取其中熵值最小的屬性(資訊獲利最大)3. 生成包含該屬性的節點4. 遞迴直到終止 ＜討論＞ ID3演算法的終極目標，就是要將決策樹中每個節點都擺上最優的環境因素( Attributes )。 $Question.$ 到底以甚麼條件決定甚麼因素要擺放於哪個節點？ $Answer.$ 資訊獎賞 or 資訊獲利( Information gain )。 資訊獲利( Information gain ) 統計該價值以檢視該環境因素置於何處來分類我們的資料，我們使用熵( entropy 又稱&quot;亂度&quot; )來定義這邊的資訊獲利( Information gain )。( 原文：Statistical quantity measuring how well an attribute classifies the data. Use entropy to define information gain. ) ID3 和 C4.5 - Information gain ( 資訊獲利 ) 與 Gain ratio定義關心其中一個環境因素( Attribute )$A$ 的資訊獲利( Information gain )我們標記為 $Gain( S, A )$，且我們關心的目標樣本群體為 $S$，其中： $$Gain( S, A ) = Entropy( S ) - \sum_{ v \in Values(A) } ( \frac{S_v}{S}Entropy(S_v) )$$ $v$ ranges over values of $A$ $S_v$: members of $S$ with $A = v$ $1^{st}$ term: the entropy of $S$ $2^{nd}$ term: expected value of entropy after partitioning with $A$ Example： PlayTennies 四個環境變因 Outlook = {Sunny, Overcast, Rain} Temperature = {Hot, Mild, Cool} Humidity = {High, Normal} Wind = {Weak, Strong} 欲看討的結果 - 開心或是不開心( Target Attributes - Binary ) PlayTennis = {Yes, No} 今天有14組訓練資料 9筆的結果是開心的 ( Positive ) 5筆的結果是不開心的( Negative ) 訓練資料表 Step 1. 計算整體的亂度( Entropy )$N_\oplus = 9, N_\ominus = 5, N_{Total} = 14$ $Entropy( S ) = -\frac{9}{14} \cdot \lg (\frac{9}{14}) - \frac{5}{14} \cdot \lg ( \frac{5}{14} ) = 0.940$ Step2. 不斷計算資訊獲利( 找亂度比較低attribute的 )，選擇最大值當作根結點 Outlook Outlook = Sunny $$N_\oplus = 2, N_\ominus = 3, N_{Sunny} = 5$$ $$Entropy(S_{Sunny}) = -(\frac{2}{5})\cdot \log_2(\frac{2}{5}) - (\frac{3}{5}) \cdot \log_2(\frac{3}{5}) = 0.971$$ Outlook = Overcast $$N_\oplus = 4, N_\ominus = 0, N_{Overcast} = 4$$ $$Entropy(S_{Overcast}) = -(\frac{4}{4})\cdot \log_2(\frac{4}{4}) - (\frac{0}{4}) \cdot \log_2(\frac{0}{4}) = 0.0$$ Outlook = Rain $$N_\oplus = 3, N_\ominus = 2, N_{Rain} = 5$$ $$Entropy(S_{Rain}) = -(\frac{3}{5})\cdot \log_2(\frac{3}{5}) - (\frac{2}{5}) \cdot \log_2(\frac{2}{5}) = 0.971$$ 計算環境因素的 Outlook 之資訊獲利 $$Gain(S, Outlook) = Entropy(S) - (N_{Sunny} / N_{total}) * Entropy(S_{Sunny})$$ $$ - (N_{Overcast} / N_{total}) * Entropy(S_{Overcast})$$ $$ - (N_{Rain} / N_{total} ) * Entropy(S_{Rain})$$ $$\Rightarrow 0.940 - (5/14) \cdot 0.971 - (4/14) \cdot 0.00 - (5/14) \cdot 0.971 = 0.246$$ Temperature Repeat process over { Hot, Mild, Cool } $$ Gain( S, Temperature ) = 0.029 $$ Humidity Repeat process over { High, Normal } $$ Gain( S, Humidity ) = 0.151 $$ Wind Repeat process over { Weak, Strong } $$ Gain( S, Wind ) = 0.048 $$ 再來，我們要找到最佳的資訊獲利( Information gain )，其中： $$Gain(S, Outlook) = 0.246$$ $$ Gain( S, Temperature ) = 0.029 $$ $$ Gain( S, Humidity ) = 0.151 $$ $$ Gain( S, Wind ) = 0.048 $$ 從亂度的點看來，似乎Outlook的亂度最低( 與宇亂度相減後剩餘比較多資訊獲利 )，所以我們選擇Outlook作為我們根結點( root node )，如下圖： 選擇了Outlook做為決策樹的根結點後，緊接著，我們可以將三種不同的Outlook作為分支，其中特別的是，Overcast狀態之中( 上圖中間綠色部分 )，全部皆為開心狀態( Positive outcome )，所以可以直接決定Overcast輸出為開心( Positive )。 Step 2. Conti. - 選擇下一個節點( 子樹的根結點 )( 從何子節點開始建子樹？ I don&#39;t know yet. ) Same steps as earlier but only examples sorted to the node are used in Gain computations.( 無法理解 ) 選一個點( 隨機？ )繼續建子樹 Outlook = Sunny $$Gain(S_{Sunny}, Humidity) = 0.97 - (3/5) \cdot 0 - (2/5) \cdot 0 = 0.97 bits$$ $$Gain(S_{Sunny}, Temperature) = 0.97 - (2/5) \cdot 0 - (2/5) \cdot 1 - (1/5) \cdot 0 = 0.57 bits$$ $$Gain(S_{Sunny}, Wind) = 0.97 - (2/5) \cdot 1 - (3/5) \cdot 0.918 = 0.019 bits$$ 由上式可以看出來Humidity的亂度最小，所以選擇之為此子樹的根。 Final Decision Tree 熵、亂度 (Entropy) 介紹在資訊理論中，熵被用來衡量一個隨機變數出現的期望值(機率與統計)。它代表了在被接收之前，訊號傳輸過程中損失的資訊量，又被稱為資訊熵。熵是對不確定性的測量。在資訊界，熵越高則能傳輸越多的資訊( 資訊越多意味著有更多的可能性 )，熵越低則意味著傳輸的資訊越少( 資訊越少意味著有更少的可能性 )。 如果有一枚理想的硬幣，其出現正面和反面的機會相等，則拋硬幣事件的熵等於其能夠達到的最大值。我們無法知道下一個硬幣拋擲的結果是什麼，因此每一次拋硬幣都是不可預測的。( 越是不可預測的結果 $\rightarrow$ 亂度越大，而這種結果，正是造成人類選擇障礙的原因，所以我們希望熵越低越好，我們可以立即做出判斷 ) $Ex1.$使用一枚正常硬幣進行拋擲，這個事件的熵是一位元，若進行n次獨立實驗，則熵為$n$，因為可以用長度為 $n$ 的位元流表示。但是如果一枚硬幣的兩面完全相同，那個這個系列拋硬幣事件的熵等於零，因為結果能被準確預測。 $Ex2.$$Let \; y \; be \; a \; Boolean \; function, and \; let \; P \; denote \; Probability.$ What is the most pure (亂度低) probability distribution? $$P(y = 0) = 1, P(y = 1) = 0$$ $$P(y = 0) = 0, P(y = 1) = 1$$ What is the most impure (亂度高) probability distribution? $$P(y = 0) = 0.5, P(y = 1) = 0.5$$ 意同於最大的亂度。 定義首先，我們可以先從簡單的看討當目前的結果最多只有兩種情況，如拋硬幣，最多只有正面或是反面，下圖$x$軸$P_\oplus$代表擲出正面的機率函數，而$y$軸則是對應的熵值，而$P_\ominus$的機率軸則是會隨著$P_\oplus$下降而上升( 兩者互補 )，但是對應到的熵值會一樣大。 $S$ is a sample of training examples( 隨機變量 ). 當今天的結果只有正與反 ( 與硬幣一樣 )時，觀察目前的隨機變量 我們令： $P_\oplus$ ( 就目前隨機變數產生的機率 ) is the portion of the positive examples ( 正面 ) in $S$. $P_\ominus$ ( 就目前隨機變數產生的機率 ) is the portion of the negative examples ( 反面 ) in $S$. Entropy ( 熵 ) measures the impurity ( 亂度 ) of $S$. 我們先定義熵值 ( Entropy ) 如下： $$ Entropy( S ) = E( I( S ) ) = E(- \ln ( P ( S ) ) ) $$ 其中，$E$為期望函數，$I( S )$是 $S$ 的資訊量（又稱為資訊本體），$I( S )$也是一個隨機變數。 所以在取硬幣的樣本( $S$ )完後，我們可以將熵值寫成： $$ Entropy( S ) = \sum_{i = 1}^{2} P(S_i)I(S_i)$$ $$ \Rightarrow -\sum_{i = 1}^{2} P(S_i)\log_{2} P(S_i) $$ $$ \Rightarrow -P_{\oplus}\log_2 P_{\oplus} - P_{\ominus}\log_2 P_{\ominus}$$ ＜Note＞ $$\sum_{i = 1}^N P_i = 1 \; and \; 0 \leq P_i \leq 1 $$ 推廣至一般式 當取自有限的樣本時，熵的公式可以表示為： $$H(X) = \sum _{i} P(x_i) \, I(x_i)=-\sum_i P(x_i)\log _b P(x_i)$$ 在這裏 $b$ 通常是$2$,自然常數 $e$，或是$10$。當$b = 2$，熵的單位是$bit$；當$b = e$，熵的單位是$nat$；而當$b = 10$,熵的單位是$Hart$。 ＜Note＞ 定義當$P_i = 0$時，對於一些 $i$ 值，對應的被加數 $0 \log_b 0$ 的值將會是 $0$，這與極限一致。 $$ \Rightarrow \lim_{p\to0+} ( p\log p ) = 0 $$ CART (Classification and Regression Tree) 見 Mr&#39; opengate - AI - Ch14 機器學習(2), 決策樹 Decision Tree 決策樹學習的常見問題 避免過度適配資料( Prevent Overfitting )首先，相較於很冗長的樹，在機器學習中其實比較偏向於比較矮的樹，然而，為何？我們可以由Occam’s Razor ( 奧坎剃刀 )得知，若有兩個假說同時都能解釋該現象，我們偏向於比較沒那麼嚴個的假說( 可以表達比較廣的概念 )。 過度配適是指模型對於範例的過度訓練，導致模型記住的不是訓練資料的一般特性，反而是訓練資料的局部特性。對測試樣本的分類將會變得很不精確。 ＜注意＞通常過度適配發生在訓練範例含有雜訊和離異值時，但當訓練數據沒有雜訊時，過度適配也有可能發生，特別是當訓練範例的數量太少，使得某一些屬性「恰巧」可以很好地分割目前的訓練範例，但卻與實際的狀況並無太多關係。 解決方案：修剪決策樹移除不可信賴的分支 事前修剪 (Prepruning) : 透過決策樹不再增長的方式來達到修剪的目的。選擇一個合適的臨界值往往很困難。 事後修剪 (Postpruning) : 子樹置換 (Subtree Replacement)：選擇某個子樹，並用單個樹葉來置換它。 子樹提升 (Subtree Raising)： 合併連續值屬性透過動態地定義新的離散值屬性來實現，即先把連續值屬性的值域分割為離散的區間集合，或設定門檻值以進行二分法。 屬性選擇指標的其他度量標準 訊息獲利 : 趨向於包含多個值的屬性 獲利比率 : 會產生不平均的分割，也就是分割的一邊會非常小於另一邊 吉尼係數 : 傾向於包含多個值的屬性，當類別個數很多時會有困難，傾向那些會導致平衡切割並且兩邊均為純粹的測試 ＜尚有其他的度量標準，也都各有利弊＞ 例題 A data set has 4 Boolean variables. What is the maximum number of leaves in a decision tree? $2^4$ To each leaf in the decision, the number of corresponding rule is 1 If a decision tree achieves 100% accuracy on the training set, then it will also get 100% accuracy on the test set? No Using information gain to pick attributes, decision tree learning can be considered A* search algorithm. No A decision tree can describe any Boolean function? Yes 補充 C4.5 演算法 C4.5演算法利用屬性的獲利比率(Gain Ratio)克服問題，獲利比率是資訊獲利正規化後的結果。求算某屬性A的獲利比率時除資訊獲 利外，尚需計算該屬性的分割資訊值(Split Information) : SplitInfoA(S)=∑t∈T|Sj||S|×log2|Sj||S| C4.5的改善： 對連續屬性的處理 改善ID3傾向選擇擁有許多不同數值但不具意義的屬性：之所以使用獲利比率(Gain Ratio)，是因為ID3演算法所使用的資訊獲利會傾向選擇擁有許多不同數值的屬性，例如：若依學生學號(獨一無二的屬性)進行分割，會產生出許多分支，且每一個分支都是很單一的結果，其資訊獲利會最大。但這個屬性對於建立決策樹是沒有意義的。 C5.0 演算法 C5.0 是 C4.5的商業改進版，可應用於海量資料集合上之分類。主要在執行準確度和記憶體耗用方面做了改進。因其採用Boosting方式來提高模型準確率，且佔用系統資源與記憶體較少，所以計算速度較快。其所使用的演算法沒有被公開。 C5.0 的優點： C5.0模型在面對遺漏值時非常穩定。 C5.0模型不需要很長的訓練次數。 C5.0模型比較其他類型的模型易於理解。 C5.0的增強技術提高分類的精度。 參考 Mr&#39; opengate - AI - Ch14 機器學習(2), 決策樹 Decision Tree Wiki - 熵 - 資訊理論 奧坎剃刀]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Decision Tree Lrearning</tag>
        <tag>Data Mining</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Longest Palindromic Substring (Manacher’s Algorithm)]]></title>
    <url>%2Fblog%2F2018%2F03%2F25%2FLongest-Palindromic-Substring-Manacher%E2%80%99s-Algorithm%2F</url>
    <content type="text"><![CDATA[Manacher&#39;s Algorithm 使用了Gusfield&#39;s Algorithm(Z algorithm)的概念，使得時間複雜度為 O(N) 。 $Z$ 陣列 $Z[i]$ 是指以 $String[i]$ 為中心的最長迴文，從中心到外端的長度，也就是說 $$String[i \ldots i-Z(i)+1] = String[i \ldots i+Z(i)-1]$$ 呈鏡面對稱。 步驟 Step1. 而這種方式無法記錄偶數長度的迴文。解決辦法是：在開頭和結尾與每個字元中間，同時插入一個沒出現過的字元( # )，如此就只剩下奇數長度的迴文了，而且也能記錄原先偶數長度的迴文。 123456 | 10 12 14 16 | 0 1 2 3 4 5 6 7 8 9 11 13 15--+-----------------------------------s | a b a a b a a bs&apos;| # a # b # a # a # b # a # a # b #Z | 1 2 1 4 1 2 7 2 1 8 1 2 5 2 1 2 1 (Step2之後會講解如何計算) Step 2宣告一個陣列 $Z[n]$ ： 其中 $n$為 $String$ 的長度。$Z[i]$ 表示以 $i$ 為中心的最長回文串長度。續上例： 1234Z(0)=1：.，由中心可以左右延伸長度1。Z(1)=2：.a.，由中心可以左右延伸長度2。Z(2)=1：.，由中心可以左右延伸長度1。Z(3)=4：.a.b.a.，由中心可以左右延伸長度4。 宣告變量 $Cur$ ：表示當前回文串最長的中心的下標（current position），初值為0。 宣告變量 $Right$ ： 表示以 $Cur$ 為中心的回文串最右一個字符所處位置。如下： Step 3開始計算 $Z[i]$ ，是運用已經算好的 $Z[j]$ ， $j &lt; i$ 。也就是指某一段已經算好的 $$String[j \ldots j-z(j)+1] = String[j \ldots j+z(j)-1]$$ 。首先找出有覆蓋到 $String[i]$ 的 $String[j \ldots j+Z[j-1]]$ 是哪一段，而且 $j+Z[j]-1$ 越右邊越好。 所以我們使用剛剛所宣告的 $Cur$ 與 $Right$ 紀錄目前能跨越最右邊的中心點在哪。 $String ：$ 再來會分成兩種情況： Case1如果 $Right$ 不能覆蓋 $String[i]$ ，表示已經算好的部份都派不上用場。從 $String[i+1]$ 與 $String[i-1]$ 開始比對，逐字比下去。 $String ：$ Case2如果 $Right$ 能覆蓋 $String[i]$，表示 $String[i]$ 也會出現在 $String[j \ldots j-Z[j]+1]$ 之中，把 $i$ 鏡射到對應的位置 $i&#39;$ (出現在 $String[j \ldots j-Z[j]+1]$的位置)。接著運用 $Z[i&#39;]$ ，也就是指 $String[i&#39; \ldots i&#39;-Z[i&#39;]+1] = String[i&#39; \ldots i&#39;+Z(i&#39;)-1]$ ，再看看是否要繼續比對字串，會分成三種子情況： Subcase1若 $String[i \ldots i+z(i&#39;)-1]$ 短少於 $Right$，那就可以直接賦予 $Z[i]$ 與 $Z(i&#39;)$ 一樣的值。 Subcase2若 $String[i \ldots i+Z[i&#39;] -1]$ 剛好貼齊於 $Right$ ，那就必須檢查未確定的部分，直接從 $String[i+Z[i&#39;]]$ 與 $String[i-Z[i&#39;]]$ 繼續比對，逐字比下去。 Subcase3若 $String[i \ldots i+Z[i&#39;]-1]$ 突出了 $Right$，根據 $Z[j]$ 可知 $String[j-Z[j]]$ 與 $String[j+Z[j]]$ 一定是不同字元，根據 $Z[i&#39;]$ 可知 $String[j-Z[j]]$ 與其鏡射位置是相同字元。對於 $i$ 來說， $String[j+Z[j]]$ 與其鏡射位置就會是不同字元，不可能形成更長的迴文，因此可以直接算出 $Z[i]$ 的值，就是 $j+z[j]-i$ 。 範例程式Palindrome - 演算法筆記_112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364char t[1001]; // 原字串char s[1001 * 2]; // 穿插特殊字元之後的tint z[1001 * 2], L, R; // 源自Gusfield's Algorithm // 由a往左、由b往右，對稱地作字元比對。int extend(int a, int b)&#123; int i = 0; while (a-i&gt;=0 &amp;&amp; b+i&lt;N &amp;&amp; s[a-i] == s[b+i]) i++; return i;&#125; void longest_palindromic_substring()&#123; int N = strlen(t); // t穿插特殊字元，存放到s。 // （實際上不會這麼做，都是細算索引值。） memset(s, '.', N*2+1); for (int i=0; i&lt;N; ++i) s[i*2+1] = t[i]; N = N*2+1;// s[N] = '\0'; // 可做可不做 // Manacher's Algorithm z[0] = 1; L = R = 0; for (int i=1; i&lt;N; ++i) &#123; int ii = L - (i - L); // i的映射位置 int n = R + 1 - i; if (i &gt; R) &#123; z[i] = extend(i, i); L = i; R = i + z[i] - 1; &#125; else if (z[ii] == n) &#123; z[i] = n + extend(i-n, i+n); L = i; R = i + z[i] - 1; &#125; else &#123; z[i] = min(z[ii], n); &#125; &#125; // 尋找最長迴文子字串的長度。 int n = 0, p = 0; for (int i=0; i&lt;N; ++i) if (z[i] &gt; n) n = z[p = i]; // 記得去掉特殊字元。 cout &lt;&lt; "最長迴文子字串的長度是" &lt;&lt; (n-1) / 2; // 印出最長迴文子字串，記得別印特殊字元。 for (int i=p-z[p]+1; i&lt;=p+z[p]-1; ++i) if (i &amp; 1) cout &lt;&lt; s[i];&#125; Palindrome - 演算法筆記_21234567891011121314151617181920212223242526272829303132char t[1001];char s[1001 * 2];int z[1001 * 2]; void longest_palindromic_substring()&#123; // t穿插特殊字元，存放到s。 int n = strlen(t); int N = n * 2 + 1; memset(s, '.', N); for (int i=0; i&lt;n; ++i) s[i*2+1] = t[i];// s[N] = '\0'; // z[0] = 1; // 無須使用，無須計算。 int L = 0, R = 0; for (int i=1; i&lt;N; ++i) // 從z[1]開始 &#123; z[i] = (R &gt; i) ? min(z[2*L-i], R-i) : 1; while (i-z[i] &gt;= 0 &amp;&amp; i+z[i] &lt; N &amp;&amp; s[i-z[i]] == s[i+z[i]]) z[i]++; if (i+z[i] &gt; R) L = i, R = i+z[i]; &#125; // 尋找最長迴文子字串的長度 int n = 0, p = 0; for (int i=1; i&lt;N; ++i) // 從z[1]開始 if (z[i] &gt; n) n = z[p = i]; cout &lt;&lt; "最長迴文子字串的長度是" &lt;&lt; (n-1) / 2;&#125; 相關題目 Timus-1297 LeetCode 參考 Manacher’s Algorithm - Simon的網路人工智慧實驗室 Palindrome - 演算法筆記]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>Palindromic</tag>
        <tag>Z Algorithm</tag>
        <tag>Substring</tag>
        <tag>Manacher&#39;s Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有關字串的名詞解釋]]></title>
    <url>%2Fblog%2F2018%2F03%2F21%2F%E6%9C%89%E9%97%9C%E5%AD%97%E4%B8%B2%E7%9A%84%E5%90%8D%E8%A9%9E%E8%A7%A3%E9%87%8B%2F</url>
    <content type="text"><![CDATA[字元 (character)： 孤單的一個符號。’7’,’1’, ’ 阿’, ’2’, ’ a’, ’ X’ 字元集 (Alphabet)： 由字元組成的集合，通常會用$\sum$表示。 字串 (String)： 由字元集中的字元構成的序列。”7122” 子字串 (Substring)： 字串中的一段連續字元。”71” in ”7122” 子序列 (Subsequence)： 字串中不需連續的一斷字元。”72” in ”7122” 前綴 (Prefix)： 一個子字串包含第一個字元。”7”, ”71”, ”712”, ”7122” in ”7122”，在這裡所有文章我會命名為前總和，方便閱讀。 後綴 (Suffix)： 一個子字串包含最後一個字元。”2”, ”22”, ”122”, ”7122” in ”7122”，，在這裡所有文章我會命名為後總和，方便閱讀。 字典序 (Alphabetical Order)： 定義字串間的大小。先定義字元間的大小：$$’\,’ &lt; ’a’ &lt; ’b’ &lt; ’c’ &lt; ’d’ &lt; …&lt; ’z’$$通常就是照著 ASCII 碼的編排順序，要注意的是 空字元 比其他字元都小 接下來從第一個位置一位一位比對，由左而右比對方小的就是比較小的字串。 後綴數組 (Suffix Array)： 將一個字串的所有後綴(後總和) ，照字典序排序後，所得的名次陣列。$$Sa[i]: 第i個後綴$$ 排名數組 (Rank Array)： 為後綴數組的逆數組。$$Ra[i]: 第 i 個後綴是第幾名$$ 最長共同前綴 (Longest Common Prefix)： 兩個字串，從第一位一位一位比對，直到不一樣就停止 $ex:$ ’712221212’ 和’712222222’ 的LCP(最長共同前綴)：’71222’。 lcp(I, J)： 對於一個字串，他的第 I 個後綴和第 J 個後綴的 LCP 有多長 LCP(I, J)： 對於一個字串，他的第 I 名後綴與第 J 名後綴的 LCP 有多長 height[i]： 對於一個字串，LCP(i − 1, i) h[i]： 對於一個字串，LCP(Ra[i] − 1, Ra[i]) 參考 建國中學 2012 年資訊能力競賽培訓講義 - 08]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Z - algorithm]]></title>
    <url>%2Fblog%2F2018%2F03%2F19%2FZ-algorithm%2F</url>
    <content type="text"><![CDATA[這個演算法可以線性時間在一段文本(text) 裡面找到所有我們欲求的段落(pattern)。 今天，當我們的文本(text)的長度為 $n$ 且欲求的段落(pattern)為 $m$時 ，搜尋只需要線性長度的時間 $O(m+n)$ 即可，雖然這個演算法需要的空間(space complexity)與時間複雜度(time complexity)都與KMP algorithm一致，但是這個演算法比起KMP algoritjm還要容易了解。 KMP algorithm：每個前綴與其後綴的次長共同前綴（最長的後綴） Z algorithm：每個後綴與母字串的最長共同前綴（單純的長度） 首先，我們需要一個 $Z$陣列($Z$ array) $Z$陣列當我們將欲檢索的文本存為一個字串 $str[0\ldots n-1]$ 時，同時也建立一個與字串一樣長的$Z$陣列。 在$Z$陣列中，第 $i$ 元素紀錄「最長共同前總和 (Longest Common Prefix)的長度」，而 LCP 的長度 是由「從 $i$ 開始的後總和 (Postfix)」與「該文本」共同決定。 ( 注意： $Z[0]$ 毫無意義可言，因為從第0個開始的後總和(Postfix) 必與原本的文本字串相同。 ) 大致上我們可以看成如下的函式： $Ex.$ 123Index 0 1 2 3 4 5 6 7 8 9 10 11 Text a a b c a a b x a a a zZ values 1 0 0 3 1 0 0 2 2 1 0 $More$ $ex.$ 12345678str = &quot;aaaaaa&quot;Z[] = &#123;x, 5, 4, 3, 2, 1&#125;str = &quot;aabaacd&quot;Z[] = &#123;x, 1, 0, 2, 1, 0, 0&#125;str = &quot;abababab&quot;Z[] = &#123;x, 0, 6, 0, 4, 0, 2, 0&#125; $Z$ 陣列如何幫助演算法加速?這個演算法的想法是將段落(pattern)與文本字串(text string)連接起來，若視段落(pattern)為「P」，視文本字串(text string)為「T」，並加上一個從未在段落與文本中出現過的字元`「\$」再產生出如「P$T」的字串。 最後，我們再產生一個屬於「P$T」的 Z陣列，在 Z陣列之中，若該 Z值等於段落(pattern)的長度，段落出現在該處。 12345678910Example:Pattern P = &quot;aab&quot;, Text T = &quot;baabaa&quot;The concatenated string is&quot;a, a, b, $, b, a, a ,b ,a, a&quot;.Z array for above concatenated string is &#123;x, 1, 0, 0, 0, 3, 1, 0, 2, 1&#125;. ^Since length of pattern is 3, the value 3 in Z array indicates presence of pattern. 如何建立 $Z$陣列最簡單的就是使用兩個迴圈，外層迴圈將整個「P\$T」跑過一遍，內層迴圈則是看看到底 i 位置的後總和與「P$T」的LCP長度為何。 $Time$ $complexity:$ $$O(n^2)$$ 我們當然可以使用另一種方法讓建立陣列的時間複雜度降低。 此演算法的關鍵在於要維護一個區間$[L \ldots R]$，$R$ 的位置代表由 $L$ 處之後可以和整個字串最長的前總和重疊到的最後一個位置( 換句話說：$[L \ldots R]$是整個字串的前綴子字串 )，若完全不重疊，則 $L$ 與 $R$相等。 1234Index 0 1 2 3 4 5 6 7 8 9 10 11 Text a a b $ a a b x a a a zZ values 1 0 0 3 1 0 0 2 2 1 0 ========= ^L ^R 步驟 ($i$ 為當前位置) 若 $i &gt; R$ ，就代表當前 $i$ 沒有經過任何「P\$S」的前綴子字串，所以重置 $L$ 與 $R$ 的位置($L = i, R = i$)，經由比對「P\$S」的前綴與 $i$ 之後的前綴，並找出最長的子字串($R$ 的位置)，計算新的 $L$ 與 $R$ 的位置，也一併將 $Z[i]$值算出來($= R - L + 1$)。 若 $i \leq R$ ，令 $K = i - L$ ，再來我們知道 $Z[i] \geq min(Z[K], R-i+1)$ 因為$String[i \ldots]$與$String[K\ldots]$共同前$R-i+1$個字元必然為[P\$T]的前綴子字串。現在有兩種情形會發生： case1： 若$Z[K] &lt; R-i+1$ ，代表沒有任何「P\$S」的前綴子字串 從 $i$ 位置開始(否則 $Z[K]$ 的值會更大)，所以也意味著$Z[i] = Z[K]$，還有區間$[L\ldots R]$不變。 case2： 若$Z[K] \geq R-i+1$，代表$String[i \ldots]$可以和$String[0\ldots]$ 繼續比對相同的字元，也就意味有可能拓展$[L \ldots R]$ 區間，因此，我們會設 $L = i$ ，接著從 $R$ 之後開始繼續比對「P\$S」的前綴子字串，最後我們會得到新的$R$，並更新$[L \ldots R]$ 區間與計算 $Z[i]$ $( = R - L + 1)$。 想要了解上述的演算法可以經由這個連結觀看動畫。 小視窗 如果一個位置 $i$ 位於之前比過的那段 $[L, R]$ 當中，他是否跟 $Z[i − L]$ 相同呢？我們可以分成三種情形： 要比的後綴根本不在以前比過的範圍$[L, R]$內 → 就去比吧！ 要比的後綴在以前比過的範圍$[L, R]$但長度未知 → 還是去比吧！ 要比的後綴在以前比過的範圍$[L, R]$但長度已知 → 直接記錄囉！ 程式碼實作 台大資工PPT by nkng12345678910void z_build(const char *S, int *Z) &#123; Z[0] = 0; int bst = 0; for(int i = 1; S[i]; i++) &#123; if(Z[bst] + bst &lt; i) Z[i] = 0; else Z[i] = min(Z[bst]+bst-i, Z[i-bst]); while(S[Z[i]] == S[i+Z[i]]) Z[i]++; if(Z[i] + i &gt; Z[bst] + bst) bst = i; &#125;&#125; Z algorithm - GeeksforGeeks123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// A C++ program that implements Z algorithm for pattern searching#include&lt;iostream&gt;using namespace std; void getZarr(string str, int Z[]); // prints all occurrences of pattern in text using Z algovoid search(string text, string pattern)&#123; // Create concatenated string "P$T" string concat = pattern + "$" + text; int l = concat.length(); // Construct Z array int Z[l]; getZarr(concat, Z); // now looping through Z array for matching condition for (int i = 0; i &lt; l; ++i) &#123; // if Z[i] (matched region) is equal to pattern // length we got the pattern if (Z[i] == pattern.length()) cout &lt;&lt; "Pattern found at index " &lt;&lt; i - pattern.length() -1 &lt;&lt; endl; &#125;&#125; // Fills Z array for given string str[]void getZarr(string str, int Z[])&#123; int n = str.length(); int L, R, k; // [L,R] make a window which matches with prefix of s L = R = 0; for (int i = 1; i &lt; n; ++i) &#123; // if i&gt;R nothing matches so we will calculate. // Z[i] using naive way. if (i &gt; R) &#123; L = R = i; // R-L = 0 in starting, so it will start // checking from 0'th index. For example, // for "ababab" and i = 1, the value of R // remains 0 and Z[i] becomes 0. For string // "aaaaaa" and i = 1, Z[i] and R become 5 while (R&lt;n &amp;&amp; str[R-L] == str[R]) R++; Z[i] = R-L; R--; &#125; else &#123; // k = i-L so k corresponds to number which // matches in [L,R] interval. k = i-L; // if Z[k] is less than remaining interval // then Z[i] will be equal to Z[k]. // For example, str = "ababab", i = 3, R = 5 // and L = 2 if (Z[k] &lt; R-i+1) Z[i] = Z[k]; // For example str = "aaaaaa" and i = 2, R is 5, // L is 0 else &#123; // else start from R and check manually L = i; while (R&lt;n &amp;&amp; str[R-L] == str[R]) R++; Z[i] = R-L; R--; &#125; &#125; &#125;&#125; // Driver programint main()&#123; string text = "GEEKS FOR GEEKS"; string pattern = "GEEK"; search(text, pattern); return 0;&#125; 建國中學 2012 年資訊能力競賽培訓講義 - 08123456789101112void Z_maker( int z[], char s[], int n )&#123; z[0] = n; int L = 0, R = 0, i, x; for( i = 1 ; i &lt; n ; i++ )&#123; if( R &lt; i || z[i-L] &gt;= R-i+1 )&#123; R &lt; i ? x = i : x = R+1; while( x &lt; n &amp;&amp; s[x] == s[x-i] ) x++; z[i] = x-i; if( i &lt; x )&#123; L = i; R = x-1; &#125; &#125; else z[i] = z[i-L]; &#125;&#125; Z algorithm - codeforces12345678910111213141516int L = 0, R = 0;for (int i = 1; i &lt; n; i++) &#123; if (i &gt; R) &#123; L = R = i; while (R &lt; n &amp;&amp; s[R-L] == s[R]) R++; z[i] = R-L; R--; &#125; else &#123; int k = i-L; if (z[k] &lt; R-i+1) z[i] = z[k]; else &#123; L = i; while (R &lt; n &amp;&amp; s[R-L] == s[R]) R++; z[i] = R-L; R--; &#125; &#125;&#125; Z algorithm1 - 日月卦長的模板庫123456789inline void z_alg1(char *s,int len,int *z)&#123; int l=0,r=0; z[0]=len; for(int i=1;i&lt;len;++i)&#123; z[i]=r&gt;i?min(r-i+1,z[z[l]-(r-i+1)]):0; while(i+z[i]&lt;len&amp;&amp;s[z[i]]==s[i+z[i]])++z[i]; if(i+z[i]-1&gt;r)r=i+z[i]-1,l=i; &#125;&#125; Z algorithm2 - 日月卦長的模板庫123456789inline void z_alg2(char *s,int len,int *z)&#123; int l=0,r=0; z[0]=len; for(int i=1;i&lt;len;++i)&#123; z[i]=i&gt;r?0:(i-l+z[i-l]&lt;z[l]?z[i-l]:r-i+1); while(i+z[i]&lt;len&amp;&amp;s[i+z[i]]==s[z[i]])++z[i]; if(i+z[i]-1&gt;r)r=i+z[i]-1,l=i; &#125;&#125; 培訓-4 字串- tioj12345678910void z_build(const char* S,int *z)&#123; z[0]=0; int bst=0; for(int i=1;S[i];i++)&#123; if(z[bst]+bst&lt;i) z[i]=0; else z[i]=std::min(z[bst]+bst−i,z[i−bst]); while(S[z[i]]==S[i+z[i]]) z[i]++; if(z[i]+i&gt;z[bst]+bst) bst=i; &#125;&#125; 例題 TIOJ 1725_Z algorithm_Massacre at Camp Happy 參考 Z algorithm - GeeksforGeeks 建國中學 2012 年資訊能力競賽培訓講義 - 08 培訓-4 字串- tioj 台大資工講義 by nkng Z algorithm - codeforces Gusfield algorithm - momo funny codes Z algorithm - 日月卦長的模板庫 待補充 KMP 字串比對演算法http://mropengate.blogspot.tw/2016/01/leetcode-kmpimplement-strstr.html]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>Z Algorithm</tag>
        <tag>Substring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[泰勒級數]]></title>
    <url>%2Fblog%2F2018%2F03%2F16%2F%E6%B3%B0%E5%8B%92%E7%B4%9A%E6%95%B8%2F</url>
    <content type="text"><![CDATA[在數學中，泰勒級數（英語：Taylor series）用無限項連加式——級數來表示一個函數，這些相加的項由函數在某一點的導數求得。泰勒級數是以於1715年發表了泰勒公式的英國數學家布魯克·泰勒（Sir Brook Taylor）來命名的。通過函數在自變量零點的導數求得的泰勒級數又叫做麥克勞林級數，以蘇格蘭數學家科林·麥克勞林的名字命名。 拉格朗日在1797年之前，最先提出帶有餘項的現在形式的泰勒定理。實際應用中，泰勒級數需要截斷，只取有限項，可以用泰勒定理估算這種近似的誤差。一個函數的有限項的泰勒級數叫做泰勒多項式。一個函數的泰勒級數是其泰勒多項式的極限（如果存在極限）。即使泰勒級數在每點都收斂，函數與其泰勒級數也可能不相等。開區間（或複平面開片）上，與自身泰勒級數相等的函數稱為解析函數。 定義 在數學上，一個在實數或複數 $a$ 在 鄰域上的無窮可微實變函數或複變函數 $f(x) $的泰勒級數是如下的冪級數 (若與原函式相等時為解析函數)： $$f(x) \simeq \sum_{n=0}^{\infty }{\frac {f^{(n)}(a)}{n!}}(x-a)^{n}$$ 而 $ f^{(n)}(a) $表示函數 $f$ 在點 $ a$ 處的 $ n$ 階導數。如果 $ a=0$ ，那麼這個級數也可以被稱為麥克勞林級數。 而多項式函數 $f(x) $ 在 $ x = a$ 時，$n $ 階的泰勒展開式 $P_{n}(x)$ 是： $$P_{n}(x) = \sum_{i = 0}^{n} \frac{ f^{(i)}(a) }{ i! }\cdot \left(x-a \right)^{i}$$ 解析函數Read more-1 (wiki)，Read more-2 (wiki) 如果泰勒級數對於區間 $ (a-r,a+r) $中的所有 $ x$ 都收斂並且級數的和等於 $ f(x) $ ，那麼我們就稱函數 $ f(x)$ 為解析的（analytic）。若且唯若一個函數可以表示成為冪級數的形式時，它才是解析的。為了檢查級數是否收斂於 $ f(x) $，通常採用泰勒定理估計級數的餘項 (數值方法)。上面給出的冪級數展開式中的係數正好是泰勒級數中的係數。 泰勒級數的重要性體現在以下三個方面： 冪級數的求導和積分可以逐項進行，因此求和函數相對比較容易。 一個解析函數可被延伸為一個定義在複平面上的一個開片上的解析函數，並使得複分析這種手法可行。 泰勒級數可以用來近似計算函數的值。 對定值 x 而言，函數的精準度會隨著多項式的次數 n 的增加而增加。 對一個固定次數的多項式而言， 確度隨著 x 離開 x=0 處而遞減。 泰勒級數列表(常用) 注意：核函數 $ x$ 為 複數 時它們依然成立！ 幾何級數(等比數列) $$\frac {1}{1-x} = \sum _{n=0}^{\infty }x^{n}\quad \forall x:\left|x\right|&lt;1$$ 二項式定理 $$(1+x)^{\alpha }=\sum _{n=0}^{\infty }C^\alpha _n \cdot x^{n}\quad \forall x:\left|x\right|&lt;1,\forall \alpha \in \mathbb {C} $$ 指數函數 $$e^{x}=\sum _{n=0}^{\infty }{\frac {x^{n}}{n!}}\quad \forall x$$ $f(x) = e^x$ 在 $x = 0$ 的泰勒展開式。 當$n = 1$時，$P_{1}(x) = 1+ \frac{\left( e^0\right)&#39;}{1!}\cdot\left( x - 0 \right)^1$ 當$n = 2$時，$P_{2}(x) = 1+ \frac{\left( e^0\right)&#39;}{1!}\cdot\left( x - 0 \right)^1 + \frac{\left( e^0\right)&#39;&#39;}{2!}\cdot\left( x - 0 \right)^2$ 當$n = 3$時，$P_{3}(x) = 1+ \frac{\left( e^0\right)&#39;}{1!}\cdot\left( x - 0 \right)^1 + \frac{\left( e^0\right)&#39;&#39;}{2!}\cdot\left( x - 0 \right)^2 + \frac{\left( e^0\right)^{(3)}}{3!}\cdot\left( x - 0 \right)^3$ $...$ 自然對數 $$\ln(1+x)=\sum _{n=1}^{\infty }{\frac {(-1)^{n+1}}{n}}x^{n}\quad \forall x\in (-1,1]$$ 牛頓插值公式的淵源Read more-1(wiki)，Read more-2(wiki) 牛頓插值公式也叫做牛頓級數，由「牛頓 前向 差分方程」的項組成，得名於伊薩克·牛頓爵士。一般稱其為連續「泰勒展開」的離散對應。 差分差分，又名差分函數或差分運算，是數學中的一個概念。它將原函數 $f(x)$ 映射到 $f(x+a)-f(x+b)$ 。差分運算，相應於微分運算，是微積分中重要的一個概念。 定義前向差分的定義為： $$\Delta_{h}^{1}[f](x) = f(x + h) - f(x)$$ $$\Delta_{h}^{n}[f](x) = \Delta_{h}^{n-1}[f](x + h) - \Delta_{h}^{n-1}[f](x)$$ $, where $ $ h =$ $ &quot;x&quot;$ $一步的間距，若無下標h，那間距h = 1。$ 前向差分函數的前向差分通常簡稱為函數的差分。對於函數 $f(x)$ ，如果在等距節點： $$x_{k}=x_{0}+kh,(k=0,1,...,n)$$ $$\Delta f(x_{k})=f(x_{k+1})-f(x_{k})$$ 則稱 $\Delta f(x)$，函數在每個小區間上的增量 $y_{k+1}-y_{k}$ 為 $f(x)$ 一階差分。 後向差分對於函數 $f(x_{k})$，如果： $$\nabla f(x_{k})=f(x_{k})-f(x_{k-1})$$ 則稱 $\nabla f(x_{k})$ 為 $f(x)$ 的一階逆向差分。 階一階差分的 差分 為二階差分，二階差分的 差分 為三階差分，其餘類推。記： $\Delta ^{n}[f](x)$為 $f(x)$的 $n$ 階差分。 $$\Delta ^{n}[f](x)=\Delta {\Delta ^{n-1}[f](x)}$$ $$=\Delta ^{n-1}[f](x+1)-\Delta ^{n-1}[f](x)$$ 其中： $$\Delta ^{2}[f](x)=f(x+2)-2f(x+1)+f(x)$$ 前向差分有時候也稱作數列的二項式變換。 由上式，再根據數學歸納法，我們最後可以得到： $$\Delta^{n}[f](x)=\sum_{i=0}^{n}C_i^n\cdot(-1)^{n-i}\cdot f(x+i)$$ 均差均差（Divided differences）是遞歸除法過程。在 數值分析 中，也稱差商（Difference quotient），可用於計算牛頓多項式形式的多項式插值的係數。 定義給定n+1個數據點 $(x_0, y_0),\ldots,(x_{n}, y_{n})$ 前向均差：$$[y_\nu] = y_, \quad \nu \in {0, \ldots, n}$$ $$[y_\nu, \ldots, y_{\nu+j}] = \frac{[y_{\nu+1}, \ldots, y_{\nu+j}] - [y_\nu, \ldots, y_{\nu+j-1}]}{x_{\nu+j} - x_\nu}, \quad \nu \in { 0, \ldots, n-j}, j \in {1, \ldots, n}$$ $ex.$ $$[y_0] = y_0$$ $$[y_0,y_1] = \frac{y_1-y_0}{x_1-x_0}$$ $$[y_0,y_1,y_2] = \frac{\mathopen[y_1,y_2]-\mathopen[y_0,y_1]}{x_2-x_0}$$ $$[y_0,y_1,y_2,y_3] = \frac{\mathopen[y_1,y_2,y_3]-\mathopen[y_0,y_1,y_2]}{x_3-x_0}$$ $$\ldots$$ $$[y_0,y_1,\dots,y_n] = \frac{\mathopen[y_1,y_2,\dots,y_n]-\mathopen[y_0,y_1,\dots,y_{n-1}]}{x_n-x_0}$$ 為了使涉及的遞歸過程更加清楚，以列表形式展示均差的計算過程： 牛頓多項式的定義給定 $k+1$ 個數據點的集合 $(x_{0},y_{0}),\ldots ,(x_{k},y_{k})$。 如果對於 $\forall i,j \in {0,...,k},i \neq j$，滿足 $ x_{i}\neq x_{j}$，那麼應用牛頓插值公式所得到的牛頓插值多項式為 $$N(x):=\sum_{j=0}^{k}a_{j}n_{j}(x)$$ 其中每個 $n_{j}(x)$ 為牛頓基本多項式（或稱插值基函數），其表達式為 $$n_{j}(x):=\prod_{i=0}^{j-1}(x-x_{i})$$ 其中 $j&gt;0$，並且 $n_{0}(x)\equiv 1$。 係數 $a_{j}:=[y_{0},\ldots ,y_{j}]$，而 $[y_{0},\ldots ,y_{j}]$表示均差。 因此，牛頓多項式可以寫作： $$N(x)=[y_{0}]+[y_{0},y_{1}](x-x_{0})+\cdots +[y_{0},\ldots ,y_{k}](x-x_{0})(x-x_{1})\cdots (x-x_{k-1})$$ 參考 維基百科 - 差分 維基百科 - 泰勒級數 維基百科 - 牛頓插值法 中山大學 - 應數碩 林瑋玲]]></content>
      <categories>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Taylor Series</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指對數]]></title>
    <url>%2Fblog%2F2018%2F03%2F15%2F%E6%8C%87%E5%B0%8D%E6%95%B8%2F</url>
    <content type="text"><![CDATA[&quot;$e^{x}$&quot; 緣起首先，要先從複利的公式開始說明： $$一年後的本利和 = \left(1+\frac{年利率}{期數} \right)^{期數}$$ 其中期數就是看多久複利一次。一個月複利一次的話期數就是12。 依上述所說，設 1 份借貸有 x 年利率，逐月複利話，則每月增加當前值的 $\frac{x}{12}$ 倍，每月總值都要乘以 $1 + \frac{x}{12}$，一年的總值為 $\left(1 + \frac{x}{12} \right)^{12}$，逐日複利的話，就是 $\left (1+ \frac{x}{365} \right)^{365}$。設年中時段數可為無限，則有如下最初由歐拉提出的指數函數定義： $$\lim_{n \to \infty}\left ( 1 + \frac{x}{n} \right )^{n}$$ 然後才真正的導出e這個數字，一開始存1元，如果年利率是100%，如果每分每秒都算利息，那麼一年後會得到的利息大約是2.71828，如果把算利息的區間縮到無限小，也就是期數變成無限大的話，就會得到 $$\lim_{n \to \infty}\left ( 1 + \frac{1}{n} \right )^{n} = e$$ 由上式我們則可以得到, $$\lim_{n \to \infty}\left ( 1 + \frac{x}{n} \right )^{n} = \lim_{n \to \infty} \left ( \left ( 1 + \frac{1}{\frac{n}{x}} \right )^{\frac{n}{x}} \right )^{x} \approx e^{x}$$ 其中， $$\lim_{n \to \infty} \left ( 1 + \frac{1}{\frac{n}{x}} \right )^{\frac{n}{x}} \approx e^{1} = e$$ 而這是它寫為 $e^{x}$ 的原因。 所以指數函數有基本的恆等式： $$e^{x+y} = e^{x} \cdot e^{y}$$ $$\parallel $$ $$\exp\left ( x + y \right ) = \exp\left ( x \right ) \cdot \exp\left ( y \right ) $$ 性質所以，正常指數該有的性質 $e$ 也都有具備，令 $\forall x, y\in \mathrm{R}$ ，則： $e^{0}=1$ $e^{1}=e$ $e^{x+y}=e^{x}e^{y}$ $e^{x \cdot y}=\left(e^{x}\right)^{y}$ $e^{-x}={1 \over e^{x}}$ 微分微分的時候需要下面這個式子： $$\lim_{n \to \infty} \left(1 + \frac{1}{n}\right) = \lim_{n \to \infty}\left(\left(1 + \frac{1}{\frac{n}{1}}\right)^{\frac{n}{1}}\right)^{\frac{1}{n}} \approx \lim_{n \to \infty} e^{\frac{1}{n}} \Rightarrow e^{\Delta x} \approx (1 + \Delta x)$$ 其中， $\lim_{n \to \infty} \left ( 1 + \frac{1}{\frac{n}{1}} \right )^{\frac{n}{1}} \approx e^{1} = e$ 當$n \to \infty$時 $\frac{1}{n}$ 可視為一個很小的量 $\Delta x$ ，也就是： $$\lim_{m \to 0} (1 + m ) \approx \lim_{m \to 0} e^{m}$$ $$(1 + 很小 ) \approx e^{很小}$$ $$(1 + \Delta x ) \approx e^{\Delta x}$$ 微分推導$$\Delta y = e^{x+\Delta x} - e^{x} = e^{\Delta x}\cdot e^{x} - 1\cdot e^{x} = (e^{\Delta x} - 1) e^{x}$$ 又因： $$e^{\Delta x} \approx (1 + \Delta x)$$ 所以： $$\Delta y \approx e^{x}\Delta x \rightarrow dy = e^{x}\cdot dx$$ 小結：$$de^{□} = e^{□}d□$$ $ex.$ $$de^{-x^2} = e^{-x^2}\cdot d(-x^2) = (-2x)\cdot e^{-x^2}\cdot dx$$ 一般化：$y = a^x $$\Rightarrow y = e^{\ln{a^{x}}} = e^{x\cdot\ln{a}} \Rightarrow $ (對 $y$ 作微分)$\Rightarrow dy= e^{x\cdot\ln{a}} \cdot \left ( x\cdot\ln{a} \right )$$\Rightarrow dy=a^x\cdot \ln a \cdot dx$ &quot;$e^{x}$&quot; 的反函數 對數函數，就是 $e^{x}$ 的反函數，也就是 $$y = \log_{e}{x} = \ln{x}$$ $$x = e^{y}$$ 對 &quot;$\ln{x}$&quot; 做微分 簡單地，我們可以推得： $y = \ln x $ $\Rightarrow x = e^y $ $\Rightarrow dx = e^y dy $$\Rightarrow dy = \frac{1}{e^y} dx = \frac{1}{x} dx $ (移&quot; $e^{y}$ &quot;項) 一般化：$y=\log_a x $ $\Rightarrow y = \frac{\ln{x}}{\ln{a}}$ $\Rightarrow dy=\frac{1}{x}\cdot \frac{1}{\ln a} \cdot dx$ &quot; $f \left( x\right) = x^x$ &quot;的微分 兩側取 &quot;$\ln$&quot;$$\ln \left( f \left( x\right) \right) = x \cdot \ln{x}$$ 對兩側微分$$\frac{f&#39;(x)}{f(x)} = \left( 1 \cdot \ln{x} \right) + \left( x \cdot \frac{1}{x}\right)$$ 對兩側乘上 &quot;$f(x)$&quot;$$f&#39;(x) = \left( \left( 1 \cdot \ln{x} \right) + \left( x \cdot \frac{1}{x}\right) \right) \cdot f(x)$$ $$\Rightarrow f&#39;(x) = \left( \ln{x} + 1 \right) \cdot \left( x^x \right)$$ 參考 成大微積分指對數函數的微分(第四週共筆) 維基百科 - e (數學常數) 維基百科 - 指數函數 中華科大 - PART 10：指數與對數微分公式彙整]]></content>
      <categories>
        <category>Calculus</category>
      </categories>
      <tags>
        <tag>Exponent</tag>
        <tag>Logarithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MathJax 文法測試]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2FMathJax-test%2F</url>
    <content type="text"><![CDATA[Admin測試$$ \lim_{n \to \infty}\left ( 1 + \frac{1}{n} \right )^{n} $$]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇測試發文]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2F%E7%AC%AC%E4%B8%80%E7%AF%87%E6%B8%AC%E8%A9%A6%E7%99%BC%E6%96%87%2F</url>
    <content type="text"><![CDATA[H1H2H3H4H5H6]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>Mark Down</tag>
      </tags>
  </entry>
</search>
