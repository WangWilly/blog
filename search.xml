<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Chinese Remainder Theorem - 中國剩餘定理專題]]></title>
    <url>%2Fblog%2F2018%2F05%2F08%2FChinese-Remainder-Theorem-%E4%B8%AD%E5%9C%8B%E5%89%A9%E9%A4%98%E5%AE%9A%E7%90%86%E5%B0%88%E9%A1%8C%2F</url>
    <content type="text"><![CDATA[Modular 反元素 (模反元素、數論倒數) 整數 \(a\) 對同於 \(n\) 之模反元素滿足以下： \[a^{-1} \equiv b \quad (mod \; n)\] 等價於 \[a \cdot b \equiv 1 \quad (mod \; n)\] ＜Note＞：整數 \(a\) 對模數 \(n\) 之模反元素存在的充分必要條件(iff)是“\(a\)”與“\(n\)”互質。 求 Modular 反元素 輾轉相除法( 歐基里德演算法 ) 原理：\(a, b\)兩整數最大公因數等於各自\(( a , b )\)與兩數相差( a - b )的最大公因數。 \(Ex\)： 252 與 105 的最大公因數為 21 \(252 = 21 \cdot 12 \quad 105 = 21 \cdot 5\)\((21 \cdot 12) - (21\cdot 5) = 21 \cdot (12 - 5) = 21 \cdot 7 = 147\) \(gcd(252, 105) = gcd(252, 147) = gcd(147, 105)\) 使用原理的過程之中，較大的數的最大公因數可以由較小的數所代表，所以繼續進行同樣的計算可以不斷縮小\(a, b\)兩數，直到最後有一數變為 \(0\) 。這時所剩下的非零數就是兩數的最大公因數。 由歐幾里得演算法的過程之中，可以推出兩數的最大公因數能用兩數的整數倍( \(\forall k \in \mathbb{Z}\) )相加來表示，承上例：\(21 = 5 \cdot 105 + (-2) \cdot 252\) Bézout’s lemma - 貝組定理 (丟番圖方程) \(\forall a, b, m \in \mathbb{Z}\) ，求未知數 \(x, y\) 的線性丟番圖方程式（稱為貝祖等式）： \[a \cdot x + b \cdot y = m\] 當 $x, y $有整數解時若且唯若( \(\Leftrightarrow\) ) \(gcd(a, b)\; | \; m\) 。此等式有解時必然有無窮多個解，每組解 \(x, y\) 都稱為貝組數可用擴展歐幾里得演算法求得，也就是說若 \(a\) 為負數\(a \cdot (-x) + b \cdot y = m\) 有整數解時\(gcd(|a|, b) \;| \; m\)。 $Ex. $ 求 \(47 \cdot x + 30 \cdot y = 1\)，求 \(x, y\)？ $47 = 1 30 + 17 $ \(\Rightarrow 30 = 1\cdot 17 + 13\) \(\Rightarrow 17 = 1 \cdot 13 + 4\)\(\Rightarrow 13 = 3\cdot 4 +1\) 我們得知 \(gcd(47, 30) = 1\)且\(1 \; | \; 1\)，所以接著改寫成「餘數等於」的形式\(17 = 1\cdot 47 - 1 \cdot 30\) \(13 = 1\cdot 30 - 1 \cdot 17\) \(4 = 1\cdot 17 - 1 \cdot 13\)\(1 = 1\cdot 13 - 3 \cdot 4\)最後再反著倒寫回去\(1 = 1\cdot 13 - 3 \cdot 4\)\(\Rightarrow 1 = 1\cdot 13 - 3 \cdot (1\cdot 17 - 1\cdot 13)\)\(\Rightarrow 1 = (-3)\cdot 17 - 4 \cdot 13\)\(\Rightarrow 1 = (-3)\cdot 17 - 4 \cdot (1\cdot 30 - 1 \cdot 17)\) $1 = 430 + (-7)17 $ \(\Rightarrow 1 = 4\cdot 30 + (-7)\cdot ( 1 \cdot 47 - 1 \cdot 30 )\) \(\Rightarrow 1 = 47 \cdot (-7) + 30\cdot 11\) 其中， \(x = -7\) 與 \(y = 11\) 為其中一組貝組數，其無限解為 \(x = -7 + 30\cdot k, y = 11 - 47 \cdot k, \forall k \in \mathbb{Z}\) Modular 反元素 若貝組等式 \(a \cdot x + b \cdot y = 1\) (若 \(\neq 1\) 則模反元素不存在)則：\(a \cdot x = 1 - b\cdot y \Leftrightarrow a\cdot x \equiv 1 \quad (mod \; b)\)所以\(a \cdot a^{-1} \equiv 1 \quad (mod \; b)\) ，此時\(x\)為\(a\)的一個模反元素，其無限表示式為\(a^{-1} = x + k\cdot b, \forall k \in \mathbb{Z}\)。 Modular - 同餘的性質 整除性 \(a\equiv b \quad (\mod m) \Rightarrow c \cdot m = a - b , c \in \mathbb{Z}\)\(\Rightarrow a \equiv b\quad ( \mod m ) \Rightarrow m \; | \; a-b\) 遞移性 若 \(a \equiv b \quad (\mod c) , b \equiv d \quad (\mod c)\)則 \(a \equiv d (\mod c)\) 保持基本運算 \(\left \{ \begin{matrix} a \equiv b (\mod m)\\ c \equiv d (\mod m)\end{matrix}\right. \Rightarrow \left\{\begin{matrix}a \pm c \equiv b \pm d (\mod m)\\ a \cdot c \equiv b \cdot d (\mod m)\end{matrix}\right.\) 放大縮小模數 令\(k \in \mathbb{Z}^+ , a \equiv b \quad (\mod m) \Leftrightarrow k \cdot a \equiv k \cdot b \quad (\mod k \cdot m)\) 費馬小定理 假設 \(a \in \mathbb{Z}\) 且 \(p\) 是質數 \(\ni gcd (a, p) = 1\)，則：\(a^p \equiv a \quad (\mod p)\)\(\Leftrightarrow a^{p-1} \equiv 1 \quad (\mod p)\) 由拉 \(\phi\) - 函數 假設 \(n \in \mathbb{Z}^+\)，定義\(\phi (n)\) 為 \(\\{ 1, 2, \ldots, n-1 \\}\) 中與\(n\)互質(coprime)的元素個數。 假設 \(m \in \mathbb{Z}, n \in \mathbb{Z}^+\)且 \(gcd(m, n) = 1\)，則：\(m^{\phi (n)} \equiv 1 \quad (\mod n)\) Chinese Remainder Theorem - 中國剩餘定理 用例子來推演整個過程： \(Ex.\) 求 \[ \left\{\begin{matrix} x &amp; \equiv &amp; 2 &amp; ( \mod 3 &amp; )\\ x &amp; \equiv &amp; 3 &amp; ( \mod 5 &amp; )\\ x &amp; \equiv &amp; 2 &amp; ( \mod 7 &amp; )\end{matrix}\right. \] 首先拆開來解方便計算： \(\left\{\begin{matrix} a_1 &amp; \equiv &amp; 2 &amp; ( \mod 3 &amp; )\\ a_1 &amp; \equiv &amp; 0 &amp; ( \mod 5 &amp; )\\ a_1 &amp; \equiv &amp; 0 &amp; ( \mod 7 &amp; )\end{matrix}\right. \Rightarrow a_1 = 35 \cdot n_1\) \(\left\{\begin{matrix} a_2 &amp; \equiv &amp; 0 &amp; ( \mod 3 &amp; )\\ a_2 &amp; \equiv &amp; 3 &amp; ( \mod 5 &amp; )\\ a_2 &amp; \equiv &amp; 0 &amp; ( \mod 7 &amp; )\end{matrix}\right. \Rightarrow a_2 = 21 \cdot n_2\) \(\left\{\begin{matrix} a_3 &amp; \equiv &amp; 0 &amp; ( \mod 3 &amp; )\\ a_3 &amp; \equiv &amp; 0 &amp; ( \mod 5 &amp; )\\ a_3 &amp; \equiv &amp; 2 &amp; ( \mod 7 &amp; )\end{matrix}\right. \Rightarrow a_3 = 15 \cdot n_3\) 接著使用同餘的保持基本運算，令\(x = a_1 + a_2 + a_3\)：\(x = a_1 + a_2 + a_3 \equiv 2 \quad (\mod 3)\)\(x = a_1 + a_2 + a_3 \equiv 3 \quad (\mod 5)\)\(x = a_1 + a_2 + a_3 \equiv 2 \quad (\mod 7)\) 計算 \(a_1\)：\[\left\{\begin{matrix} a_1 &amp; \equiv &amp; 2 &amp; ( \mod 3 &amp; )\\ a_1 &amp; \equiv &amp; 0 &amp; ( \mod 5 &amp; )\\ a_1 &amp; \equiv &amp; 0 &amp; ( \mod 7 &amp; )\end{matrix}\right.\] ，\(a_1 = 35\cdot n_1 \equiv 2 \quad(\mod 3)\)不好運算，轉成\(b_1 = 35\cdot m_1 \equiv 1 \quad(\mod 3)\)\(\Rightarrow 35 = 11 \cdot 3 + 2\)\(\Rightarrow 3 = 2 \cdot 1 + 1\)\(1 = 1 \cdot 3 - 1 \cdot 2\)\(\Leftrightarrow 1 = 1 \cdot 3 - 1 \cdot ( 1 \cdot 35 - 11 \cdot 3 )\)\(\Leftrightarrow 1 = (-1) \cdot 35 + 12 \cdot 3\)令\(m_1 = -1 + 3 \cdot k\) (模反元素)所以，\(b_1 = 35\cdot m_1 \equiv 1 \quad(\mod 3) \Leftrightarrow 2 \cdot b_1 \equiv 2 \cdot 35 \cdot m_1 \equiv 2 \cdot 1 \quad (\mod 3)\)則可以令\(a_1 = b_1\cdot 2\)，取\(k = 1, b_1 = 35 \cdot 2 = 70 \Rightarrow a_1 = 140\) 計算\(a_2\)：\[\left\{\begin{matrix} a_2 &amp; \equiv &amp; 0 &amp; ( \mod 3 &amp; )\\ a_2 &amp; \equiv &amp; 3 &amp; ( \mod 5 &amp; )\\ a_2 &amp; \equiv &amp; 0 &amp; ( \mod 7 &amp; )\end{matrix}\right.\] ，\(a_2 = 21\cdot n_2 \equiv 3 \quad(\mod 5)\)不好運算，轉成\(b_2 = 21\cdot m_2 \equiv 1 \quad(\mod 5)\)\(\Rightarrow 21 = 4 \cdot 5 + 1\)\(1 = 21 \cdot 1 - 4 \cdot 5\)令\(m_2 = 1 + 5 \cdot k\) (模反元素)所以，\(b_2 = 21 \cdot m_2 \equiv 1 \quad (\mod 5) \Leftrightarrow 3 \cdot b_2 = 3\cdot 21 \cdot m_2 \equiv 3 \cdot 1 \quad (\mod 5)\)所以令\(a_2 = b_2 \cdot 3\)，取\(k = 0, b_2 = 21 \cdot 1 = 21 \Rightarrow a_2 = 63\) 計算\(a_3\)：\[\left\{\begin{matrix} a_3 &amp; \equiv &amp; 0 &amp; ( \mod 3 &amp; )\\ a_3 &amp; \equiv &amp; 0 &amp; ( \mod 5 &amp; )\\ a_3 &amp; \equiv &amp; 2 &amp; ( \mod 7 &amp; )\end{matrix}\right.\] ，\(a_3 = 15\cdot n_3 \equiv 2 \quad(\mod 7)\)不好運算，轉成\(b_3 = 15\cdot m_3 \equiv 1 \quad(\mod 7)\)\(\Rightarrow 15 = 2 \cdot 7 + 1\)\(1 = 15 \cdot 1 - 2 \cdot 7\)令\(m_3 = 1 + 7 \cdot k\) (模反元素)所以，\(b_3 = 15 \cdot m_3 \equiv 1 \quad (\mod 7) \Leftrightarrow 2 \cdot b_3 = 2\cdot 15 \cdot m_3 \equiv 2 \cdot 1 \quad (\mod 7)\)所以令\(a_3 = b_3 \cdot 2\)，取\(k = 0, b_3 = 15 \cdot 1 = 15 \Rightarrow a_3 = 30\) 最後計算 \(x = a_1 + a_2 + a_3 = 233\)，驗算\(\left\{\begin{matrix} 233 &amp; \equiv &amp; 2 &amp; ( \mod 3 &amp; )\\ 233 &amp; \equiv &amp; 3 &amp; ( \mod 5 &amp; )\\ 233 &amp; \equiv &amp; 2 &amp; ( \mod 7 &amp; )\end{matrix}\right.\) ，OK。 常見題目類型 Type 1 (模數都互質) \(Ex.\)(99 政大) \(\left\{\begin{matrix} x &amp; \equiv &amp; 5 &amp; ( \mod 7 &amp; )\\ x &amp; \equiv &amp; 4 &amp; ( \mod 9 &amp; )\\ x &amp; \equiv &amp; 3 &amp; ( \mod 13 &amp; )\end{matrix}\right.\) Type 2 (模數不全是互質) \(Ex.\)(97 台科大) \(\left\{\begin{matrix} x &amp; \equiv &amp; 1 &amp; ( \mod 2 &amp; )\\ x &amp; \equiv &amp; 2 &amp; ( \mod 3 &amp; )\\ x &amp; \equiv &amp; 8 &amp; ( \mod 15 &amp; )\end{matrix}\right.\) Type 3 (模數重疊) \(Ex.\)(97 高師大) \(\left\{\begin{matrix} x &amp; \equiv &amp; 1 &amp; ( \mod 3 &amp; )\\ x &amp; \equiv &amp; 13 &amp; ( \mod 16 &amp; )\\ x &amp; \equiv &amp; 73 &amp; ( \mod 81 &amp; )\end{matrix}\right.\) Type 4 (矛盾 - 無解) \(Ex.\)(97 台科大 - 改) \(\left\{\begin{matrix} x &amp; \equiv &amp; 1 &amp; ( \mod 2 &amp; )\\ x &amp; \equiv &amp; 2 &amp; ( \mod 3 &amp; )\\ x &amp; \equiv &amp; 10 &amp; ( \mod 15 &amp; )\end{matrix}\right.\)]]></content>
      <categories>
        <category>Number Theory</category>
      </categories>
      <tags>
        <tag>Modular</tag>
        <tag>Mod</tag>
        <tag>Inverse</tag>
        <tag>Chinese Remainder Theorem</tag>
        <tag>模數</tag>
        <tag>模反元素</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra - 重點分析 - LU分解]]></title>
    <url>%2Fblog%2F2018%2F05%2F07%2FLinear-Algebra-%E9%87%8D%E9%BB%9E%E5%88%86%E6%9E%90-LU%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Linear Algebra - 重點分析 LU分解 LU 分解的外表看似平淡無奇，但它可以用來解線性方程，逆矩陣和計算行列式，堪稱是最具實用價值的矩陣分解式之一。 令 \(A\) 為一個 \(n\cdot n\) 階矩陣。LU 分解是指將 \(A\) 表示為兩個 \(n \cdot n\) 階三角矩陣的乘積 \[ A = L\cdot U \] 其中 \(L\) 是下三角矩陣，\(U\) 是上三角矩陣，如下例， \[ \begin{bmatrix}3 &amp; 1 &amp; 2 \\ 6 &amp; -1 &amp; 5 \\ -9 &amp; 7 &amp; 3\end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 2 &amp; 1 &amp; 0 \\ -3 &amp; 4 &amp; 1 \end{bmatrix} \begin{bmatrix}3 &amp; -1 &amp; 2\\ 0 &amp; 1 &amp; 1\\ 0 &amp; 0 &amp; 5 \end{bmatrix} \] 高斯消去法可以通過一連串的矩陣乘法來實現。每一個基本列運算等同於左乘一個基本矩陣，而對應列取代的基本矩陣 \(E_{ij}\) 的 \((i, j)\) 元即為 \(-l_{ij}\)。 消去程序可用下列矩陣乘法表示： \[ E{32}E{31}E_{21}A=U \] 因為基本矩陣 \(E_{ij}\) 都是可逆的 \[ A=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}U=LU \] 存在性 然而，並非任何可逆矩陣都具有 LU 分解形式，例如：\(A=\begin{bmatrix} 0&amp;2\\ 1&amp;3 \end{bmatrix}\)。假若 \(A\) 可以寫為 \[ A=LU=\begin{bmatrix} 1&amp;0\\ l_{21}&amp;1 \end{bmatrix}\begin{bmatrix} u_{11}&amp;u_{12}\\ 0&amp;u_{22} \end{bmatrix} \] 則必有 \(u_{11}=0\)， \(U\) 是不可逆的，這與 為 \(LU=A\) 可逆矩陣的事實相互矛盾。矩陣 之所 \(A\) 以不存在 分解的\(LU\) 原因在於 \(0\) 占據了 \((1,1)\) 元，但軸元必須不為零才能產生乘數。根據這項觀察，即知可逆矩陣 \(A\) 的 LU 分解存在條件是：列運算過程中，\(0\) 不得在軸元位置。萬一碰上零軸元的情況，還是有補救辦法，那就是使用列交換運算設法產生其他非零軸元，不過 LU 分解要修改成 \(PA=LU\)，\(P\) 是排列矩陣。例如， \[ \begin{aligned} PA&amp;=\begin{bmatrix} 0&amp;1\\ 1&amp;0 \end{bmatrix}\begin{bmatrix} 0&amp;2\\ 1&amp;3 \end{bmatrix}=\begin{bmatrix} 1&amp;3\\ 0&amp;2 \end{bmatrix}\\ &amp;=\begin{bmatrix} 1&amp;0\\ 0&amp;1 \end{bmatrix}\begin{bmatrix} 1&amp;3\\ 0&amp;2 \end{bmatrix}=LU.\end{aligned} \] 應用 最後討論 LU 分解的應用。LU 分解不僅僅只是記錄消去過程，它還有一個非常重要的實際用途：LU 分解具備快速求解線性方程 \(A\mathbf{x}=\mathbf{b}​\) 的良好結構。一旦得到了可逆矩陣 \(A​\) 的 LU 分解 \(A=LU​\)，我們大可把 \(A​\) 拋棄，將 \(A\mathbf{x}=\mathbf{b}​\) 改為 \(L(U\mathbf{x})=\mathbf{b}​\)，再令 \(\mathbf{y}=U\mathbf{x}​\)，原線性方程等價於兩組三角形系統： \[ \begin{aligned} L\mathbf{y}&amp;=\mathbf{b}\\ U\mathbf{x}&amp;=\mathbf{y}. \end{aligned} \] 接著使用兩次迭代即可得到解。上例中，先以正向迭代解出 \(\mathbf{y}\)， \[ \left[\!\!\begin{array}{rcc} 1&amp;0&amp;0\\ 2&amp;1&amp;0\\ -3&amp;4&amp;1 \end{array}\!\!\right]\begin{bmatrix} y_1\\ y_2\\ y_3 \end{bmatrix}=\left[\!\!\begin{array}{r} 10\\ 22\\ -7 \end{array}\!\!\right]\Rightarrow\begin{cases} y_1=10&amp;\\ y_2=-2y_1+22=2&amp;\\ y_3=3y_1-4y_2-7=15&amp; \end{cases} \] 再以反向迭代解出 ， \[ \left[\!\!\begin{array}{crc} 3&amp;-1&amp;2\\ 0&amp;1&amp;1\\ 0&amp;0&amp;5 \end{array}\!\!\right]\begin{bmatrix} x_1\\ x_2\\ x_3 \end{bmatrix}=\left[\!\!\begin{array}{r} 10\\ 2\\ 15 \end{array}\!\!\right]\Rightarrow\begin{cases} x_1=(x_2-2x_3+10)/3=1&amp;\\ x_2=-x_3+2=-1&amp;\\ x_3=15/5=3&amp; \end{cases} \] 對於 階矩陣 ，LU 分解耗費的乘法運算量大約是 \(\mathbf{O}(\frac{1}{3}n^3)\)，與高斯消去法相同。這個數字其實不算太糟，因為兩個 \(n\) 階方陣相乘就使用了 \(n^3\) 次運算。另外，正向迭代或反向迭代的運算量都只有\(\mathbf{O}(\frac{1}{2}n^2)\) ，遠比 LU 分解來的少。所以如果只要解出單一線性系統 ，直接用消去法化簡增廣矩陣 \(\begin{bmatrix} A\vert\mathbf{b} \end{bmatrix}\) 和 LU 分解的兩段式解法兩者之間並沒有多大差別，但如果稍後還要解多個係數矩陣相同但常數向量改變的系統 ，LU 分解便能夠派上用場。舉例來說，LU 分解可以用來計算逆矩陣 \(A^{-1}\)。將矩陣方程 看成三個線性方程： \[ A\mathbf{x}_1=\begin{bmatrix} 1\\ 0\\ 0 \end{bmatrix},~ A\mathbf{x}_2=\begin{bmatrix} 0\\ 1\\ 0 \end{bmatrix},~ A\mathbf{x}_3=\begin{bmatrix} 0\\ 0\\ 1 \end{bmatrix} \] 解出的未知向量 \(\mathbf{x}_i\)，\(i=1,2,3\)，就是逆矩陣 \(A^{-1}\) 行向量。LU 分解還可以用來計算 \(n\times n\) 階行列式。根據矩陣乘積的行列式可乘公式 \[ \det A=\det(LU)=(\det L)(\det U) \] 三角矩陣的行列式等於主對角元乘積，因此 \(\mathrm{det}L=1\)，推論 \[ \det A=\det U=\displaystyle\prod_{i=1}^nu_{ii} \] 方陣 \(A\) 的行列式即為消去法所得到的上三角矩陣 \(U\) 主對角元之積 (關於其他行列式計算方法的介紹，請見“Chiò 演算法──另類行列式計算法”)。 參考 LU 分解| 線代啟示錄]]></content>
      <categories>
        <category>Linear Algebra</category>
        <category>重點分析</category>
      </categories>
      <tags>
        <tag>LU分解</tag>
        <tag>LU Decomposition</tag>
        <tag>重點</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Patent Claim - 專利聲明]]></title>
    <url>%2Fblog%2F2018%2F05%2F05%2FPatent-Claim-%E5%B0%88%E5%88%A9%E8%81%B2%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Patent Claim - 聲明專利 專利的「異議」與「舉發」 異議 對於審定公告之專利原則上，任何人得自審定公告之次日起3個月內提起異議。 用於撤銷該專利之審定。 舉發 對於以取得之專利，得對之提起舉發。 用於撤銷「專利權」，使專利自使部存在。 ＜Note＞：有無仿冒？以專利角度看待，抄襲仿冒不是重點，重點是有無落入對方的專利申請範圍內。 專利範圍之記載要件 包含必要的技術特徵。 明確，使相關領域具通常知識者，可以明確瞭解其意義，不生疑義。 所載的發明符合產業利用性、新穎性與進步性。 為發明說明所支持。 結構 ( P + T + B ) 前言 ( Preamble ) 連接詞 ( Transition ) 主體 ( Body ) 獨立項與附屬項 ★★★★★ 獨立項：載明申請專利之標的及其實施之必要技術特徵。 ☆☆☆☆★ 附屬項：包含所依附項目之全部技術內容，並說明依附請求項以外之技術特徵。 ＜Note＞：獨立項遠重要於附屬項。 \(Ex.\) 一種杯子，其包含方形杯體及連接其上之把手。如請求項1之杯子，其中該杯體高度為10公分。如請求項2之杯子，其中該把手是透明的。 請求項 ( Claim ) 一般建議涵蓋不同範圍的請求項 最廣 中等 - 最小：最終銷售產品。 thedifferentsetofclaim 選擇一個「女友」？ 請根據以下條件以請求項( Claim )的方式要求。 覺得她長的漂亮 身高165公分以上，必須比我矮5公分，所以要在175公分以下 瘦瘦的 懂得化妝 最好長髮飄逸 即使是生氣時，也要很溫柔 要當太太不一樣，要會做家事，會煮飯，喜歡小孩 年輕的未婚女性 \(Ex.\) 獨立項( 範圍 ) 一名女性人類( P )，包含( T )下述之特性( B )： 覺得她長的漂亮\(\rightarrow\)五官特徵與松島菜菜子差異0%~20%之間。 身高165公分以上，必須比我矮5公分，所以要在175公分以下\(\rightarrow\)身高165公分至175公分之間。 瘦瘦的\(\rightarrow\)以身高推算後之標準體重再減10%~20%之間。 懂得化妝\(\rightarrow\)修過認證化妝課程20小時以上。 即使是生氣時，也要很溫柔\(\rightarrow\)喜歡不聽話的0歲到16歲人類。 要當太太不一樣，要會做家事，會煮飯，喜歡小孩\(\rightarrow\)喜歡也愛做被分配到的工作，但不要求薪水。 年輕的未婚女性\(\rightarrow\)年齡16歲到20歲的未婚人類。 附屬項( 範圍 ) 如專利範圍第1項所述之女性人類，更包括下述特性： 最好長髮飄逸\(\rightarrow\)長髮過肩30公分以上。 ＜Note＞：申請專利範圍用字要具體。越抽象的形容詞\(\rightarrow\)越難寫長得漂亮\(\rightarrow\)五官特徵與松島菜菜子差異0%20%之間\(&lt;br&gt;\rightarrow\)五官特徵與20歲松島菜菜子，經微軟發行的臉部辨識軟體，辨識之後差異0%20%之間 申請專利範圍所列的條件，『都』必須符合，才算是在專利範圍所主張的範圍之內。 來了1個大美女、但少了一隻耳朵、指甲長且銳利、手常過膝 解決： 加入 Claim 3 一名女性人類( P )，包含( T )下述之特性( B )： 除了剛剛所述另外加入 有兩隻健全耳朵、且可辨識聲波。 四肢健全、雙手自然下垂、長度介於腰繫之間。 每根手指指甲長度在1.2~2公分之間。 來了個日本女生，但是個六指琴魔 修正 Claim： 有兩隻健全耳朵、且可辨識聲波。 四肢健全、雙手自然下垂、長度介於腰繫之間。 每隻手有五根手指頭、每根手指指甲長度在1.2~2公分之間。 thedifferentsetofclaim2 外加一個Claim 4涵蓋Claim 1～3 一名女性人類，包含以下特性。 - 一名女性人類，目前未婚 thedifferentsetofclaim3 申請專利範圍之結構 前言 ( Preamable ) 連接詞 ( Trainsition ) 開放式：包含、包括、其特徵在於。(Comprise, contain, characterized by) 封閉式：由…組成。( Consisting of ) 半開放式：基本上由…組成。( Consisting essentially of ) 主體 ( Body ) 前言 描述所請發明之標的型態： 一種組合物… 一種裝置… 一種方法… 限制條件? 一種用於載裝飲料( 用途限定 )的杯子… 一種具有兩分裝空間( 結構限定 )的杯子… 一種杯子… 連接詞 開放式：包括、包含(comprise, contain) 表示所申請範圍包含主體中未記載之元件、成分、或步驟。 例如：一種組合物，包括成分A及B。 ＜Note＞：專利範圍涵蓋度最廣。 半開放式：基本上由…組成(consisting essentially of) 不排除實質上無影響力之成分。 例如：一種組合物，基本上由成分A及B組成。 ＜Note＞：專利範圍涵蓋度中度。 封閉式：由…組成(consisting of) 僅包含主體中記載之元件、成分、或步驟。 例如：一種組合物X，由成分A及B組成。 ＜Note＞：專利範圍涵蓋度最低。 主體 記載所請發明的主要特徵 標的請求項：物品或物質的結構特徵。 方法請求項：步驟。 \(Ex.\) claimpractice1 Claim A claimpractice2 Claim B claimpractice3 Claim C claimpractice4 Claim D claimpractice5 Claim E claimpractice6 upload successful ＜Note＞：限制條件越多，權力範圍越小。專利說明書的所有部分，都是為『申請專利範圍』而活。 theeffectofthesizeofclaim 範圍可以無限擴大嗎？ 專利範圍當然是越大越好。 『發明之必要條件』與『既有技術』，專利說明書的具體實例也會影響專利範圍的大小。 最後，範圍無限擴大的專利的問題在能否通過。]]></content>
      <categories>
        <category>Technical Patent Study</category>
      </categories>
      <tags>
        <tag>Patent</tag>
        <tag>Claim</tag>
        <tag>專利聲明</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Patent Specification - 專利說明書]]></title>
    <url>%2Fblog%2F2018%2F05%2F05%2FPatent-Specification-%E5%B0%88%E5%88%A9%E8%AA%AA%E6%98%8E%E6%9B%B8%2F</url>
    <content type="text"><![CDATA[Patent Specification - 專利說明書 不能只提出要解之問題，要寫出『如何辦到』。專利說明書要說明『如何辦到』。 想法：趁聖誕老公公發禮物時抓住他。 作法：需要具體機制。 狗應該要有專門的馬桶。 作法：如何設計一個狗狗專用的馬桶。 說明書的內容 patentspecificationcontent INID專利書目識別碼 Internationally agreed Numbers for the Identification of bibliographic Data 二位阿拉伯數字組成的數字，作為辨識專利公報之書目資料項目的重要指標 國際專利分類號 1971年由Strasburg國際專利協定所制定，並由世界智慧財產權組織(WIPO)每五年修訂一次並出版，其分類目的在便於專利資料的整理、查索及管理等運用。 目前最新版本為第八版，已於2006.01公佈並使用，並區分為核心版與進階版。 IPC從第一版的47,263個分類增加到第七版的67,634個分類，第八版的分類總數增加至約70,000個分類(進階版)，且分類架構已有大幅度的變動。 第七版以前約每五年更新一次，從第八版起，核心版將縮短為三年一次，進階版則隨時更新。 IPC分類之編排大致以 部(Section) 類(Class) 次類(Subclass) 主目(Group) 次目(Subgroup) 五大階層編排中，每一階層彼此之間具有從屬關係。 Section - 部 theclassficationofIPC_section Class - 類 upload successful Santa Calus Detector Patent 5523741 thesantacalusdetector1 thesantacalusdetector2 thesantacalusdetector3 upload successful 小結論 subsummary1 subsummary2 代號意義 [10] 文件識別 Identification of the patent, SPC, or patent document [11] 文件號碼 Number of the patent , SPC, or patent document [12] 文件種類 Plain language designation of the kind of document [13] 文件種類代碼 Kind-of-document code according to WIPO Standard ST. 16 [15] 專利修正資訊 Patent correction information [19] 文件發行單位 WIPO Standard ST. 3 Code, or other identification, of the office or organization publishing the document Santa Claus Detactor Patent 5523741 santacalusdetector5 [20] 專利申請登記項目 Data concerning the application for a patent or SPC [21] 申請號 Number(s) assigned to the application(s) [22] 申請日 Date(s) of filing the application(s) [23] 其他日期 Other Date(s), including date of filing complete specification following provisional specification and date of exhibition [24] 工業產權生效日 Date from which industrial property rights may have effect [25] 申請案最初提出時使用語文 Language in which the published application was originally filed [26] 最初申請案之語文 Language in which the application is published Santa Claus Detactor Patent 5523741 santacalusdetector6 [30] 國際優先權 Data relating to priority under the Paris Convention [31] 優先權申請號 Number(s) assigned to priority application(s) [32] 優先權申請日 Date(s) of filing of priority application(s) [33] 優先權申請國家 WIPO Standard ST. 3 code identifying the national industrial property office allotting the priority application number or the organization allotting the regional priority application number ; for the international applications filed under the PCT, the code “WO” is to be used [35] 優先權申請日期 For priority filings under regional or international arrangements, the WIPO Standard ST. 3 code identifying at least one country party to the Paris Convention for which the regional orinternational application was made [40] 公開日期 Date(s) of making available of the public [41] 未經審查尚未獲准專利的說明書提供公眾閱覽或複印的日期 [42] 經審查但尚未獲准專利的說明書提供公眾閱覽或複印的日期 [43] 未經審查之出版日期(公開日) Date of making available to the public by printing or similar process of an unexamined patent document, on which no grant has taken place on or before the said date [44] 經審查未獲權之出版日期 [45] 經審查已獲權之公告日(公告日) Date of making available to the public by printing or similar process of an examined patent document on which grant has taken place on or before the said date [46] 僅有申請專利範圍的出版日期 [47] 獲准專利說明書提供公眾用覽或複製的日期 Date of making available to the public by viewing, copying on request, a patent document on which grant has taken place on or before the said date Santa Claus Detactor Patent 5523741 upload successful [50] 技術資料 Technical Information [51] 國際專利分類 International Patent Classification or, in the case of a design patent, as referred to in subparagraph 4(c) of this Recommendation, International Classification for Industrial Designs [52] 本國專利分類 Domestic or national Classification [54] 發明名稱 Title of the invention [56] 先前技術文件明細 List of prior art document, if separate from descriptive text [57] 摘要或申請專利範圍 Abstract or claim [58] 檢索範圍 Field of search Santa Claus Detactor Patent 5523741 santacalusdetector8 [60] 與申請有關之法律文件 References to other legally or procedurally related domestic or previously domestic patent documents including unpublished applications therefore [61] 追加關係 Earlier document to which this is an addition [62] 分割關係 Earlier application form which the present document has been divided out [63] 延續關係 Continuations [64] 再發行關係 Document being reissued [65] 同申請案, 先前公開於其他國家之文件 Previously published document concerning same application [66] 取代關係 Document for which this is a substitute [70] 人事項目 Identification of parties concerned with the patent ot SPC. [71] 專利權人/申請人 Name(s) of applicant(s) [72] 發明人 Name(s) of inventor(s) if known to be such [73] 受讓人 Name(s) of grantee(s), holder(s),assignee(s), or owner(s) [74] 代理人 Name(s) of attorney(s) agent(s) [75] 發明人兼申請人 Name(s) of inventors who is (are) also applicant(s) [76] 發明人兼申請人及受讓人 Name(s) of inventors who is (are) also applicant(s) and grantee(s). Santa Claus Detactor Patent 5523741 santacalusdetector9 [80] 有關國際條約之資料識別 Identification of data related to International Conventions other than the Paris Convention, and to legislation with respect to SPCs. [81] PCT國際申請指定國 [82] PCT選擇國 Elected State(s) according to the PCT [83] 微生物寄存資料 Information concerning the deposit of microorganisms [84] 專利指定國 Designed Contracting States under regional patent conventions [85] 符合PCT第23條第1款或第40條第1款之規定而開始國內程序之日期 [86] PCT申請案相關資料 [87] PCT公開資料(公開號、公開語言、公開日) [88] 檢索報告延遲公佈日期 [89] 發明人證書，或基於CMEA協定對於發明之保護文件號碼及原始國 Santa Claus Detactor Patent 5523741 santacalusdetector10]]></content>
      <categories>
        <category>Technical Patent Study</category>
      </categories>
      <tags>
        <tag>Patent</tag>
        <tag>專利</tag>
        <tag>Specification</tag>
        <tag>專利說明書</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Content of Patent 專利看討]]></title>
    <url>%2Fblog%2F2018%2F05%2F05%2FContent-of-Patent-%E5%B0%88%E5%88%A9%E7%9C%8B%E8%A8%8E%2F</url>
    <content type="text"><![CDATA[專利看討 不是攻擊對手，而是保護自己(和公司)的權力。 專利制度 中華民國專利法 分為3章共156條 ＜專利法(1)＞：為鼓勵、保護、利用發明、新型及設計之創作，以促進產業發展，特制定本法。 ＜專利法(2)＞：本法所稱專利，分為 發明專利。 (新發明) 新型專利。 (新型) 設計專利。 (新式樣) formoftypeofpatent 什麼可以成為專利？ 若認為能賺錢的發明才是發明，會非常難達成。專利保護的概念，往往比大部分人想的都要廣，就算看起來不像是可受專利保護的概念，換個角度想也常能受專利保護。 thepatentofconcept 「改良」也算是發明 發明電燈泡的只有一個人？ 事實上，愛迪生的電燈泡出現之前70年，已經知道用白金當燈絲，但非真空。 只要有發明的概念 並有具體的作法。 不能成為專利的「概念」 動植物的新品種。 如基因改造過的玉米、新品種的老鼠。 人體或動物疾病之診斷、治療或治療方法。 心臟移植手術的方法。 ＜Note＞： 藥物、製藥方法、醫療器材等這些不包含在此。 科學原理或數學方法本身 \(E = M\cdot C^2\) ＜Note＞： 利用科學原理或是數學方法是可以成為專利的。\(Ex.\) MP3 壓縮技術、加解密技術。 純為人類智力的規則或方法。\(Ex.\)：如何節稅、如何開連鎖店。 屬著作權保護的美術、文學與音樂創作。 負面表列 一般是否屬於專利會保護的概念，只要看當地專立法所列的『負面表列』即可。 ＜專利法(24)＞：下列各款，不予發明專利： 動植物及生產動、植物之主要生物學方法。但微生物學之生產方法 ，不在此限。 人類或動物之診斷、治療或外科手術方法。 妨害公共秩序或善良風俗者。 專利在國際的考慮 各國價值觀不同，所核准的專利範圍也不盡然相同。慎選區域，因為所費不斐，最好以市場大小及競爭對手的區域為基準考量。]]></content>
      <categories>
        <category>Technical Patent Study</category>
      </categories>
      <tags>
        <tag>Patent</tag>
        <tag>Concept</tag>
        <tag>專利看討</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Patent Applying 專利申請]]></title>
    <url>%2Fblog%2F2018%2F05%2F05%2FPatent-Applying-%E5%B0%88%E5%88%A9%E7%94%B3%E8%AB%8B%2F</url>
    <content type="text"><![CDATA[申請專利 申請流程 申請→18個月後公開→3年內請求實體審查→實體審查→審定公告3個月→審查確定(無異議或異議不成立)→領證→( 舉發 )。 申請日 決定誰可以獲得勝利！ 申請日之取得 專利法(25)：申請發明專利，由專利申請權人備具申請書、說明書、申請專利範圍、摘要及必要之圖式，向專利 專責機關 申請之。 專利法(25)：申請發明專利，以申請書、說明書、申請專利範圍及必要之圖式齊備之日為『申請日』。 申請文件 需求 申請書 說明書 發明名稱 發明說明應明確且充分揭露發明之內容。 申請專利範圍各請求向以簡潔之方式記載。 圖示 ＜Note＞：若申請有『生物材料專利』，申請人應將該生物材料寄存於指定機構。 專利生效時間線 timelineofpatentapplying 專利權人 發明人 對於這個發明有貢獻的人。 專利申請人 提出專利申請的人，可以是公司、組織( 法人 )或是個人( 自然人 )。 ＜Note＞： 一定要填寫發明人，用來表彰發明人的貢獻，若發明人的填寫有捏造的事實，即便取得專利權也可能被撤銷專利。 誰是專利權人？ 真正擁有該專利權的人是「專利申請人」。 審查 專利提出申請 (繳費) 請求審查 (再繳費) 申請後必須要在 2~5年( 大部分是3年 )提出，否則專利視同撤回。＜用意＞： 考慮專利是否具備價值。’ ＜Note＞：美國的專利申請，一律自動審查，無請求審查制度，所有費用都包含在提出專利申請時的繳費。 始得權力日 專利權有效自申請日起算。 侵權日自專利審查完成後起算。 timelineofpatentapplying2 早期公開制度( Limited Early Publication System ) 『早期公開，請求審查』 為大部分國家所採用。 在申請日後，18個月會在『公開公報』上刊登申請專利的內容。＜目的＞：使同產業提早新技術內容，以及避免重複研發相同技術。 暫准專利權( Provisional right ) 如果在早期公開至專利核准期間，發生侵權行為，日後專利核准之後，專利權人可以要求暫准專利權其間的損失。 公眾審查制度( Public review system ) 有些國家規定，在核准專利過後的3～6個月，會公開讓民眾確認這項專利的合格性，待時間過後繳交「專利領證費」並領證，接著，每年還需繳交「專利維持費」。 歐盟 [x] 公眾審查制 ( 3～6個月 ) 美國 [ ] 公眾審查制 核准後直接領證。 撤銷專利 在領到證書之後，任何人都可以主張撤銷你的專利。 可能的原因 沒有符合「專利要件」。 不同意審查委員的看法。 提出當初審查委員沒有看討的先前技術( 習之技術 )。 先申請主義( Double Patenting ) 誰將獲得台灣專利？ doublepatenting ( 乙君 ) ＜先申請主義＞ 現在沒有所謂的＜先發明主義＞；依我國專利法(31)：相同發明有二以上之專利申請案時，僅得就其最先申請者准予發明專利。 ＜Note＞：先申請先贏。 屬地主義 只有在該國的專利核准過的專利，才能在該國享有專利權，其餘國家的專利權皆無法在該國產生任何作用。 國際、國內優先權 Right of Priority - 國際優先權 誰將獲得台灣專利？ rightofpriority ( 乙君 ) 甲君未聲明＜國際優先權＞！ 專利申請人就相同發明，在他國提出第一次申請案後，於特定期間內向我國提出之專利申請案得以主張「優先權』，換句話說，以國外第一次申請案之申請日為『優先權日』，並據以作為審查是否符合專利之基準日。 ＜專利法(28)＞：申請人就相同發明在與中華民國相互承認優先權之國家或世界貿易組織會員(WTO)第一次依法申請專利，並於第一次申請專利之日後十二個月內，向中華民國申請專利者，得主張優先權。 ＜專利法(29)＞：依前條規定主張優先權者，應於申請專利同時聲明下列事項： 第一次申請之申請日。 受理該申請之國家或世界貿易組織會員。 第一次申請之申請案號數。申請人應於最早之優先權日後十六個月內，檢送經前項國家或世界貿易組織會員證明受理之申請文件。 rightofpriority2 Right of Domestic Priority - 國內優先權 誰將獲得台灣專利？ rightofdomesticpriority ＜專利法(30)＞：申請人基於其在中華民國先申請之發明或新型專利案再提出專利之申請者 ，得就先申請案申請時說明書、申請專利範圍或圖式所載之發明或新型，主張優先權。 ＜Note＞：但有下列情事之一，不得主張之 自先申請案申請日後已逾十二個月者。 upload successful 曝光條款 ＜專利法(22)＞：可供產業上利用之發明，無下列情事之一，得依本法申請取得發明專利。 申請前已見於刊物者。 申請前已公開實施者。 申請前已為公眾所知悉者。 ＜Note＞：發明雖無前項各款所列情事，但為其所屬技術領域中具有通常知識者依申請前之先前技術所能輕易完成時，仍不得取得發明。 ＜專利法(22)＞：申請人有下列情事之一，並於其事實發生後六個月內申請，該事實 非 屬第一項各款或前項不得取得發明專利之情事。 因實驗而公開者。 因於刊物發表者。 因陳列於政府主辦或認可之展覽會者。 非出於其本意而洩漏者。 情境題 甲君獲得專利 situationquestion1 乙君無法獲得專利 upload successful 甲君獲得專利 upload successful 沒有人可獲得專利 ( 超過六個月的申請期 ) upload successful 審查 中華民國由智慧財產局依該申請是否具備專利要件決定。 符合專利要件→授予專利(核准) 不符合專利要件→應予駁回(核駁) 核駁 申請人對於『不給專利之審定』，不服者可以申請再審查。 ＜專利法(48)＞：發明專利申請人對於不予專利之審定有不服者，得於審定書送達後二個月內(60天內)備具理由書，申請再審查。但因申請程序不合法或申請人不適格而不受理或駁回者，得逕依法提起行政救濟。 行政救濟 對於『再審查』不服之一方：應提起訴願。( 向經濟部為之 ) 對於『訴願』不服之一方：應提起行政訴訟。( 向行政院為之 ) 起訴：高等行政法院 上訴：最高行政法院 專利權人 專利權人是誰？ situationquestion5 ＜專利法(7)＞： 受雇人於職務上所完成之發明、新型或設計 其專利申請權及專利權屬於雇用人，雇用人應支付受雇人適當之報酬。 但契約另有約定者( 合約 )，從其約定。 ＜Note＞：發明人具有『姓名表示權』。 ＜專利法(8)＞：受雇人於非職務上所完成之發明、新型或設計，其專利申請權及專利權屬於受雇人。 ＜專利法(7)＞：一方出資聘請他人從事研究開發者，其專利申請權及專利權之歸屬 依雙方契約約定 契約未約定者，屬於發明人、新型創作人或設計人。 ＜Note＞： 出資人得實施其發明、新型或設計之權力。 ＜Note＞： 美國專利的專利申請人一定是發明人，因此當職務上之發明，則在申請的同時，送交一份『讓與書』，以便權力由個人轉到公司。 Contract of Employment 聘用合約書 contractofemplement 補充 專利費用預估參考 - 以新台幣計(工研院資料) feeofpatent 輔大專利申請 patentapplyingofFJU patentofapplyingofFJU2 參考 中華民國專利公報檢所解析 早期公開を英語で・英訳 - 英和辞典・和英辞典 Weblio辞書 雙語詞彙]]></content>
      <categories>
        <category>Technical Patent Study</category>
      </categories>
      <tags>
        <tag>Patent</tag>
        <tag>Apply</tag>
        <tag>專利</tag>
        <tag>申請</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Technical Patent Study - 科技專利專題]]></title>
    <url>%2Fblog%2F2018%2F05%2F04%2FTechnical-Patent-Study-%E7%A7%91%E6%8A%80%E5%B0%88%E5%88%A9%E5%B0%88%E9%A1%8C%2F</url>
    <content type="text"><![CDATA[科技專利專題 Technical Patent Study - 概觀 首先，要了解專利就必須先思考到底是甚麼東西、有什麼用？ 瞭解專利對於企業發展及生存的重要性。 知道自己或團隊研發成果，哪些可以申請專利。 透過專利檢索快速學習他人技術。 避免侵害他人的專利。 撰寫優質的專利說明書(申請專利的技術文件)。 智慧財產權 Intellectual Property Right 智慧財產權，是一種無體財產權，係指人類精神活動之成果而能產生財產上之價值者，由法律所創設的一種權利，包括： 著作權 商標權 專利權 工業設計 積體電路布局 (IC) 鄰接權 植物種苗 商業秘密 不公平競爭 intalletualpropertyform 創作保護主義 創作保護主義，僅有創作之事實，即取得著作權。但為期權利明確及有完整之著作權記錄，得兼採登記主義，而登記機關改歸中央圖書館。但著作物登記並非著作權之取得要件，僅推定其權利為真實。至於著作權之移轉或處分之限制，以及以著作權為標的之質權的設定、移轉、變更或消滅或處分之限制，則非經登記，不得對抗第三人。 其保護僅及於該著作之表達(Expression)，而不及於其所表達之思想(Idea)、程序、製程、系統、操作方法、概念、原理、發現。 ＜小見解＞：上述的「表達(Experssion)」就是已經動手做出的有形實體(書畫)，或無形創作(音樂、文章、程式碼)。 著作權 著作權法之沿革 前清宣統 2年（ 1910 年）頒布之著作權律，係我國第一部之成文著作權法，其採註冊保護主義，著作物經註冊給照者，受著作權保護（著作權律第 4 條）。民國建立後，著作權律仍然沿用。北洋政府嗣於 1915 年頒布一部著作權法。國民政府亦於 1928年頒布著作權法，期間於 1944年、1949年、1964年、1985年、1990年、1992年、1993年、1998年、2001年、2003年及2004年均有修正。1985年修法前，作者必須申請註冊登記，始取得著作權，1985年修法後，改為著作人於著作完成時，享有著作權，係創作保護主義或稱自然發生主義。註冊或登記均僅具存證之性質，並非取得著作權之要件，倘當事人對於權利有爭執時，應由當事人自行提出證據證明之，並由司法機關依具體個案調查事實認定之，不應以著作權登記簿謄本之核發，作為認定著作權有無之唯一證據。而智慧財產局自1998年1月23日起，已不再受理著作權登記業務，倘原登記或註冊事項如有變更亦無從辦理更新。 著作權侵害 判斷是否受侵害時須考慮的因素有二： 被侵害的標的必須是表達而非思想。 被告須有接觸(Access)或實質相似(Substantial Similarity)的抄襲(Copying)行為。 ＜小見解＞：接觸(Access)就是該被告有可能接處到原創作者的任何途徑，所以有些創作者會在淨室(Clean Room)創作，防止任何的接觸可能性。 如何保護著作權 「舉證之所在、敗訴之所在」，如何舉證該著作是自己的原創是件很重要的事，用白話文來說，就是在創作時要無時無刻保留自己的手稿，來證明創作的靈感。法律是講求證據的，如果研發過程中未留下任何紀錄，則明明是贏的官司也可能會打輸。 有些科技公司會有研發紀錄簿，證明創作的理念。 建立淨室(Clean Room)，意旨個隔離所有可能的接觸。 小結論 『專利保護概念，著作權不保護概念』 舉例 \(Ex1.\) 微積分或力學原理，教師製作磨課師課程可以參考原文教科書，另行自編教材並解說，但在作業或測驗方面，就必須使用原文書的例題及習題內容了，這部分可以自由利用嗎？如果台灣書商無法授權，又該如何？ 原文教科書的例題及習題，仍是受著作權法保護，教師必須自行製作題目，否則就只能取得授權。 台灣書商無法給予課本內容之授權，通常是因為他只有賣書的權利，沒有內容授權的權利，必須與出版社連繫。若出版社無權授權，就要與著作人聯絡。真的找不到，就只能自己創作，才是安全的。 這就說明了，編一本書是很辛苦的，內容及例題和解答，都是智慧成果，所以書才會賣得很貴。磨課師課程的內容製作也很辛苦，若想少辛苦一點，就是付錢取得他人既有成果來使用，這無法單是以標明出處，就可取代授權的。 \(Ex2.\) patentexample1 patentexample2 patentexample3 patentexample4 patentexample5 patentexample6 審查主義 指創作完成後，須向 主管機關 提出申請、並經主管機關審查通過後，始能取得權利者。 中華民國的主管機關為『經濟部智慧財產局』 美國的主管機關為『專利商標局』 日本的主管機關為『特許廳』 大陸的主管機關為『知識產權局』 商標 商標（英語：Trademark）是識別某商品、服務或與其相關具體個人或企業的顯著標誌，可以是圖形或文字，也可以聲音、氣味或立體圖來表示。 標記 在採商標註冊標示國家，如美國，圖形「®」表示某個商標經過註冊，並受法律保護，稱作「主要註冊」（Principal Register）。圖形「™」常用來指某個標誌未經註冊通過而作為商標使用，僅具描述（merely descriptive）性質，可申請「輔助註冊」（supplemental register）避免日後其他類似商標註冊混淆，但標示使用上不限商標是否註冊通過。另外，尚有「℠」的服務商標。 目前兩岸三地商標法規中，中國大陸有明確規範「®」及圈內中文化的「注」，香港和台灣則無。 專利 專利流程 3 階段 upload successful 專利申請 繼續閱讀 參考 五南文化 - 著作法參考書 著作權之侵害與救濟：第六章 著作權侵害之救濟的基本問題 認識智慧財產權 - 國立政治大學 Wiki - 商標]]></content>
      <categories>
        <category>Technical Patent Study</category>
      </categories>
      <tags>
        <tag>Patent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integration by parts - 分部積分法以及常用的分部積分]]></title>
    <url>%2Fblog%2F2018%2F05%2F01%2FIntegration-by-parts-%E5%88%86%E9%83%A8%E7%A9%8D%E5%88%86%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%9A%84%E5%88%86%E9%83%A8%E7%A9%8D%E5%88%86%2F</url>
    <content type="text"><![CDATA[分部積分法是種積分的技巧常應用於微積分數學與數值分析之中。它是由微分的乘法定則和微積分基本定理推導而來的。其基本思路是將不易求得結果的積分形式，轉化為等價的但易於求出結果的積分形式。 規則 當 \(u = u(x)\) 、 \(du = u&#39;(x)dx\) 、 \(v = v(x)\) 與 \(dv = v&#39;(x)dx\) ， 那分部積分就可以寫為： \[ \int_a^b u(x)v&#39;(x)dx=[u(x)v(x)]_a^b-\int_a^b u&#39;(x)v(x)dx \] \[ \Leftrightarrow u(b)v(b)-u(a)v(a)-\int_a^b u&#39;(x)v(x)dx \] 或是以更常見的簡寫： \[ \int u \; dv = uv - \int v \; du \] 定理 假設 \(u(x)\) 與 \(v(x)\) 是兩個連續可導函數 (continuously differentiable functions). 由乘法定理 (product rule) 可知(用來布尼茲表示法 Leibniz’s notation)： \(\frac{d}{dx} ( u(x) \cdot v(x) ) = \frac{d(u(x))}{dx}\cdot v(x) + u(x)\cdot \frac{d(v(x))}{dx}\) 對兩側求不定積分： \(uv = \int (\frac{d(u(x))}{dx}\cdot v(x) + u(x)\cdot \frac{d(v(x))}{dx})dx\)\(\Leftrightarrow \int d(u(x))\cdot v(x) + \int u(x)\cdot d(v(x))\)\(\Rightarrow \int u \; dv = uv - \int v \; du\) 常用的分部積分 \(\int \ln(x) dx = x\ln(x) - x + C\) \(\int \ln(x)dx\) 令 \(u = \ln(x)\) 、 \(dv = dx\) ， 則 \(du = \frac{1}{x} dx\) 、 \(v = x\) 帶入： \(\int \ln(x) dx = \ln(x) \cdot x - \int x\cdot \frac{1}{x}dx\) \(\Leftrightarrow \ln(x)\cdot x - \int(1)dx\) \(\Leftrightarrow \ln(x)\cdot x - x + C\) \(\int \log (x) dx = x\cdot \log (x) - \frac{x}{\ln 10} + C\) 令 \(u = \log (x)\) 、 \(dv = dx\) ， 則 \(du= d(\log (x)) \Leftrightarrow d(\frac{\ln x}{\ln 10})\) ＜乘法定理＞： 上微下不微 + 下微上不微 \(\Leftrightarrow d(ln x)\cdot \frac{1}{\ln 10} + \ln x\cdot d(\ln 10)\) \(\Leftrightarrow \frac{1}{x}\cdot \frac{1}{\ln 10} + \ln x \cdot 0\) \(\Leftrightarrow \frac{1}{x\ln 10}\) 接著 \(v = x\) 。 \(\int \log x dx = \log x \cdot x - \int x \cdot \frac{1}{x \ln 10} dx\) \(\Leftrightarrow x\cdot \log x - \int \frac{1}{\ln 10} dx\) \(\Leftrightarrow x\cdot \log x - x\cdot\frac{1}{\ln 10} + C\) 參考 Wiki - 分部積分法 Math2.org Math Tables: Integral ln(x) Derivative of Log X]]></content>
      <categories>
        <category>Calculus</category>
      </categories>
      <tags>
        <tag>Integration</tag>
        <tag>Partial Integration</tag>
        <tag>Integration By Parts</tag>
        <tag>logarithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Artificial Neural Networks 類神經網路學習]]></title>
    <url>%2Fblog%2F2018%2F04%2F25%2FArtificial-Neural-Networks-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%AD%B8%E7%BF%92%2F</url>
    <content type="text"><![CDATA[Artificial Neural Networks 類神經網路概論( 未完成 ) 簡介 類神經網絡是一種受生物學啟發而產生的一種模擬人腦的學習系統。 Youtube - 介紹神經元與類神經的關係 neuralgraph ↑ 神經元示意圖( *synapse：比重 ) 對於神經(neuron)我們有一個簡單的抽象：每個神經元是與其他神經元連結在一起的，一個神經元會受到多個其他神經元狀態的衝擊，並由此決定自身是否激發。 神經細胞透過輸入神經樹由其它神經細胞輸入脈波訊號後，經過神經細胞核的處理，其處理大約是： 將收集到的訊號作加總 非線性轉換 產生一個新的脈波信號 如果這個訊號夠強，則新的脈波信號會由神經軸傳送到輸出神經樹，再透過神經節將此訊號傳給其它神經細胞。值得注意的是：當訊號經過神經節後，由於神經節加權值的影響，其訊號大小值會改變。 神經網絡裡的結點相互連結決定了輸入的數據在裡面經過怎樣的計算。我們可以通過大量的輸入，讓神經網絡調整它自身的連接情況從而總是能夠得到我們預期的輸出。 neuralgraph2 ＜比較＞電腦裡的模擬神經網路的架構需具備： 模擬頭腦神經的連結( 包含模擬突觸、細胞本體( 隱藏層 )、軸突 ) 每個神經節點 實數的輸入與輸出 極大量的資訊 遷移學習( Transfer learning ) 基礎概念 - 周莫烦 - 站在巨人的肩膀上, 迁移学习 Transfer Learning 實際應用( Python ) - 周莫烦 - 迁移学习 Transfer Learning 什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？ 感知器 ( Perceptron ) 設有 \(n\) 維輸入的單個感知機( 從其他類神經元接收到的資訊 )，\(a_1\) 至 \(a_n\) 為 \(n\) 維輸入向量的各個分量，\(w_1\) 至 \(w_n\) 為各個輸入分量連接到感知機的權值( 比重 )，\(w_0\) 為偏置( 常數 )，一個神經元( Cell Body )分成兩個步驟，第一個 \(\sum\) 為彙總資料，後面那個 \(f(.)\) 為傳遞函數( 圖上的函數是“Sign function” )，判斷最後輸出的值， 最後以純量輸出( 1 or -1 )。 \[ Input: x_1, x_2, …,x_n \] \[ Output: 1 or -1 \] perceptrongraph 類神經元示意圖 ＜注意＞\(w_0\)不是伴隨其他的資訊傳進神經元的，而是因為某些演算法的需求，而另外多加的一個閾值( 正負常數值 )。 $ _{i = 0}^n W_i X_i$又稱為淨輸入( Net Input )，可處理線性組合的假說空間( Hypotheses )。 sgn_triggerfunction 上圖為此神經元判斷的觸發函數( 每個神經元的判斷都不盡相同，此為其中一種 )，帶入剛剛所算的淨輸入，計算輸出( -1就是判斷為無反應的狀況 )。 權值(\(w\))：如果當前神經元的某個輸入值權值為零，則當前神經元激發與否與這個輸入值無關；如果某個輸入值的權重為正，它對於當前神經元的激發值產生正影響。反之，如果權重為負，則它對激發值產生負影響。 偏移量(\(w_0\))：它定義了神經元的激發臨界值在空間上，它對決策邊界(decision boundary) 有平移作用，就像常數作用在一次或二次函數上的效果。感知器表示為輸入向量與權向量內積時，偏置被引申為權量，而對應的輸入值為 1。 決策邊界(decision boundary)：設輸入向量與權向量的內積為零，可得出 n+1 維的超平面。平面的法向量為 w，並經過 n+1 維輸入空間的原點。法向量指向的輸入空間，其輸出值為+1，而與法向量反向的輸入空間，其輸出值則為−1。故可知這個超平面定義了決策邊界，並把輸入空間劃分為二。 decisionboundery 激勵函數(activation function)：激勵函數代表神經元在什麼輸入情況下，才觸發動作。 activationfunction 感知器可以「學習」的函數 singlelayertwoinputperceptron Consider a 2-input perceptron ( 感知器 ) : It outputs 1 iff \[ o( x_1, x_2 ) = ( w_0+w_1 \cdot x_1+w2 \cdot x2 &gt; 0 )? \] equivalent to \[ o( x_1, x_2 ) = sgn( w_0+w_1 \cdot x_1+w2 \cdot x2 ) \] What weights represent \(AND (x1, x2)\)? \(w_0 = -0.8, w_1 = w_2 = 0.5\)\(o( x_1, x_2 ) \Rightarrow sgn(-0.8 + 0.5 \cdot x_1 + 0.5 \cdot x_2 )\) weighttorepresentAND What weights represent \(OR (x1, x2)\)? \(w_0 = 0.3, w_1 = w_2 = 0.5\)\(o( x_1, x_2 ) = sgn(0.3 + 0.5 \cdot x1 + 0.5 \cdot x2 )\) weighttorepresentOR What weights represent \(NOT (x1, x2)\)? \(w_0 =0.0, w_1 = -1.0, w_2 = 0\)\(o(x_1) = sgn( 0.0 –1.0x_1)\) weighttorepresentNOT What weights represent \(XOR (x1, x2)\)? Not possible. possibletorepresentXOR ＜NOTE＞Not linearly separable \(\rightarrow\) Can not be represented by a single percepton. Solution: use multilayer networks. multilayertorepresentXOR How to Determine a Weight Vector? 如何決定權重向量？ 在類神經網路學習的過程中，最重要的就是權重向量( Weight Vector )，因為這就是決定到時候感知器( Perceptrons )能不能做出正確預測( correct \(\pm 1\) output )的關鍵依據。 通常來說，都會給定一組訓練範例( Trainning example )，而且，每個元素裡必定會含有輸入( Input )與輸出( Output )。 \(( x_1, x_2, x_3, \ldots , x_{n-1}, x_n )\) 是訓練範例中會給的資訊。 \(+1 \; or \; -1\) 為 \(( x_1, x_2, x_3, \ldots , x_{n-1}, x_n )\) 的已知輸出( Target value )。 而我們的目標就是將 \(( w_1, w_2, w_3, \ldots , w_{n-1}, w_n )\) 訓練出來。 解決的演算法有很多，在這邊只討論其中兩個： The perception trainnin rule Gradient decent ( or call the delta rule ) Perceptron Training Rule \(t = c(x_1, x_2, x_3, ..., x_n )\) 是我們已知道的結果( 1 or -1 )。 \(o：\)對於訓練資料$ (x_1, x_2, x_3, …, x_n ) $以感知器( Perceptron )測試後出來的結果( 1 or -1 )。 ＜Note＞：Here o is the output of Perceptron, not the target value. 所以 $ ( t - o ) $ 為此時感知器( Perceptron )的誤差，然後藉由我們設定的 \(\eta\) 函式判斷要對目前的 \(w\) 修正多少值。 演算法 Initialize weights (w0, w1, w2, x3, …,wn ) to random values Loop through training examples： \(w_i \leftarrow w_i + \Delta w_i\) Where \(\Delta w_i = \eta (t-o) \cdot x_i\) and \(\eta\) is a learning rate (small positive value, e.g., 0.1) Given training data set \[ D = \{ ( \vec{x}, t ) \} \] 123456789//Initialize all weights w_i to random valuesw[] &lt;- random valuesWHILE not all examples correctly predicted DO FOR each training example x = (x_1, x_2, x_3, ..., x_n ) in D Compute current output o ( x_1, x_2, x_3, ..., x_n ) FOR i = 0 to n // always let x_0=1 w_i w_i + eta(t - o) * x_i ＜Note＞ If (t-o) = 0, no change in weight. 輪過一遍所有訓練資料，稱之為一個時代( Epoch )，若一個時代過後還有 \(w_i\) 是錯誤的就繼續修改 \(w_i\) ，直到某個時代所有的 $ w_i $ 可以讓 $ x_i $ 輸出正確。 ＜注意＞：如果訓練資料是線性可分離( XOR就不可線性分離 )，且\(\eta\)是小於1的很小的值，那麼一定最後可以在有限的世代找到最後的感知器( Perceptron )。 倘若今天的資料是無法被線性分離的改如何處理？ - Approach 1: 建立一個演算法可以努力找到逼近值。 E.g. gradient descent method ( 梯度下降法 ) - Approach 2: 建立不同架構或多層( Multilayer networks )結構的神經網路以突破限制。 Gradient Descent Youtube - Gradient Decent 介紹 我們需要在 \((n+1)\) 維的假說向量空間( Hypotheses Space )中搜索最合適( Best fit )的權值向量，我們需要有一定的規則指導我們的搜索，採用沿著梯度反方向往下走的方法，就稱為「梯度下降法」(Gradient Descent)。這種方法可以說是一種「貪婪演算法」(Greedy Algorithm)，因為它每次都朝著最陡的方向走去，企圖得到最大的下降幅度。即使訓練資料是不可線性分離的( Not lineary separable )，最後這個演算法還是會收斂在極趨近於目標想法的銓重向量停止。 ＜注意＞： Least square為最常用來檢測誤差的方法。 為了要計算梯度，我們不能採用不可微分的 sign() 步階函數，因為這樣就不能用微積分的方式計算出梯度了，而必須改用可以微分的連續函數 sigmoid()，這樣才能夠透過微分計算出梯度。 \[ E(w) = \frac{1}{2} \sum_{d \in D} ( t_d - o_d ) \] 上面公式中\(D\)代表了所有的輸入案例( 或者說是樣本 )，\(d\)代表了一個樣本實例，\(o_d\)表示感知器的輸出，\(t_d\)代表我們預想的輸出。 首先，我們先看看權重( Weight vector )向量 \(w\) 的梯度( Gradient )為何： \[ \bigtriangledown E( w ) = \frac{\partial E}{\partial w} = ( \frac{\partial E}{\partial w_0}, \frac{\partial E}{\partial w_1}, \ldots, \frac{\partial E}{\partial w_n} ) \] ＜注意＞： 梯度 是一個裡面所有元素為對 \(E\) 以對每個 \(w_i\) 偏微分的向量。且這個向量指向的地方為最上坡之處。( 如下圖紅色處顯示，而下方黑色箭頭則表示該梯度投影下來所對應的方向 ) gradient 所以 Gradient Descent 就是該點梯度的 反方向 ，也就是最下坡的方向，\(i.e. \; -\bigtriangledown E(w)\)。 這樣目標就明確了，欲在假說空間找到一組權值 \(w\) 讓這個誤差的值最小，顯然我們用誤差對權值求導將是一個很好的選擇，導數的意義是提供了一個方向，沿著這個方向改變權值，將會讓總的誤差變大，更形象的叫它為梯度。 既然梯度確定了E最陡峭的上升的方向，那麼梯度下降的訓練法則是： \[ \vec{w_i} \leftarrow \vec{w_i} + \Delta \vec{w_i}, \quad where \; \Delta \vec{w_i} = \eta \frac{\partial E}{\partial w_i} \] gradientgraph \(E.x.\) Example: two weights: \(w = (w_0, w_1)\) Error surface \(E\) is parabolic (by definition) Single global minimum Arrow: negated gradient at one point Steepest descent along the surface examplegradient For the least square error function, gradient is easy to calculate: \[ \bigtriangledown E( w ) = \frac{\partial E}{\partial w} = \frac{1}{2} \cdot \frac{\partial \sum_{d \in D} (t_d - o_d)^2}{\partial w_i} = \frac{1}{2} \sum_{d \in D}\frac{\partial (t_d - o_d)^2}{\partial w_i} \] \[ \Rightarrow \frac{1}{2} \cdot \sum_{d \in D} (2 \cdot (t_d - o_d)\frac{\partial( t_d - o_d )}{\partial w_i}) = \sum_{d\in D}((t_d - o_d)\frac{\partial(t_d - w\cdot x_d)}{\partial w_i}) \] \[ \Rightarrow \sum_{d\in D} ((t_d - o_d)(-x_{id})) \] 依上述，公式就可以簡化成： \[ \Delta w_i = -\eta \frac{\partial E}{\partial w_i} \] \[ and \] \[ \frac{\partial E}{\partial w_i} = \sum_{d \in D}((t_d - o_d)(-x_{id})) \] 最後公式變成： gradientdecentfinalformula 參考 Mr’ opengate - AI - Ch16 機器學習(4), 類神經網路 Neural network Wikipedia - 人工神經網路 基礎概念 - 周莫烦 - 站在巨人的肩膀上, 迁移学习 Transfer Learning 實際應用( Python ) - 周莫烦 - 迁移学习 Transfer Learning 什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Neural Networks</tag>
        <tag>Gradient Decent</tag>
        <tag>Perceptron trainning rule</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Supervised learning 監督式學習 - Concept Learning 概念學習]]></title>
    <url>%2Fblog%2F2018%2F04%2F20%2FSupervised-learning-%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-br-Concept-Learning-%E6%A6%82%E5%BF%B5%E5%AD%B8%E7%BF%92%2F</url>
    <content type="text"><![CDATA[Supervised learning 監督式學習 - Concept Learning 概念學習 machinelearningconcept 機器學習簡介( 節錄自 Mr’ OpenGate ) 機器學習是近20多年興起的一門多領域交叉學科，涉及機率論、統計學、逼近論、凸分析、計算複雜性理論等多門學科。機器學習理論主要是設計和分析一些讓計算機可以自動「學習」的演算法。機器學習算法是一類從資料中自動分析獲得規律，並利用規律對未知資料進行預測的算法。 機器學習已廣泛應用於數據挖掘、計算機視覺、自然語言處理、生物特徵識別、搜尋引擎、醫學診斷、檢測信用卡欺詐、證券市場分析、DNA序列測序、語音和手寫識別、戰略遊戲和機器人等領域。 ＜定義＞ 機器學習定義如下 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. \(Ex.\) T(任務)：將郵件分類為垃圾或非垃圾。 E(經驗)：觀察目前信箱的信是把哪些種類的郵件標記為垃圾，而哪些是非垃圾。 P(效能)：被正確分類成垃圾或非垃圾的郵件的數量。 supervisedlearningworkflow 一種歸納的方式 從已知的現象訓練機器判斷常態的結果，而已知的現象只有是非之結論。(所以用這個方法訓練出來的智能只能判斷對錯) 所以以這種方法訓練的智能我們可以看待它是一個布林函數(Boolean-valued Funciotn)。 輸入 - 欲判斷的狀態 (Attributes)。 輸出 - 對(TURE)、錯(FALSE)。 learningfunction 環境狀態( Attribute )、學習目標( Target Concept ) Example from book：Enjoy sport conceptlearningenjoysportdata 六個會影響的環境狀態 ( Attribute ) 四個案例 ( Instance ) 學習目標：判斷當時是否很享受運動。\(EnjoySport = \{Yes, No\}\) 環境狀態( Attribute ) 每個事件(instance)發生時會引響結果的基本元素。 倘若現在有一事件會被影響的環境狀態為 \(N\) 個。 每個環境狀態可能出現的狀態個數為 \(n_i\) 。(第 \(i\) 個環境狀態) (Nominal values; symbolic values; Discretized values)(? 待了解) 案例( Instance ) - 已知結果的一群環境狀態 令 \(x\) 為已知結果的一群環境狀態。 那我們稱所有可能產生的案例為一個空間(Space)並稱它 \(X\)。 若 \(M\) 等於此空間 \(X\) 的大小，則 $ M = n_1 n_2 n_{N-1} n_N$。 我們其實可以將 \(M\) 視為此環境狀態所有組合的方法數。 學習目標( Target Concept ) 要學到的想法(在這裡我們給機器訓練的想法也可視為一個函式)。 \(c(x)=1 \qquad if \; EnjoySport=Yes\) \(c(x)=0 \qquad if \; EnjoySport=No\) \(c\) 是一個定義再案例空間(Instance Space)的布林函數。 \(c:X \rightarrow \left\{ 0, 1 \right\}\) 訓練集合 “\(D\)” 所有已知想法的案例集合 \(Ex.\) \[ &lt;x_1, c(x_1)&gt;, &lt;x_2, c(x_2)&gt; \ldots &lt;x_m, c(x_m)&gt; \] 假說( Hypotheses ) - 在這裡我們標記為 \(H\) 定義：所有環境狀態( Attributes )之限制( Constraints ) 的交集( Conjunction )。 限制( Constraints )的種類 Specific value ( 針對值 ) \(\qquad e.g. \; ( sky = sunny )\) Don’t care value ( 不在意值 ) \(\qquad e.g. \; ( sky = &quot;？&quot; )\) No value allow ( 無值 ) \(\qquad e.g. \; ( sky = &quot;\phi&quot; )\) 若今天有一案例( Instance )符合了我們的假說，也就是說它每個環境狀態( Attributes )全部都不逾越我們假說中的所有限制( Constraints )。 \(Ex.\) \[ h \leftarrow &lt; Sunny, Warm, ?, Strong, ?, ? &gt; \] 假說空間( Hypotheses Space )的大小 語意上來說( Syntactically distinct number ) \[ M_H = ( n_1 + 2 ) \cdot ( n_2 + 2 ) \cdot \ldots \cdot ( n_{N-1} + 2 ) \cdot ( n_{N} + 2 ) \] \[ ( Two \; more \; &quot;values&quot; \; have \; been \; added, &quot;?&quot; and &quot;\phi&quot; ) \] 實際上來說( Semantically distinct number ) \[ M_H = 1+ ( n_1 + 1 ) \cdot ( n_2 + 1 ) \cdot \ldots \cdot ( n_{N-1} + 1 ) \cdot ( n_{N} + 1 ) \] 因為如果該假說的限制交集裡，有一個以上的“”存在於集合中，代表所有的案例( Instances )絕對都不可能不逾越我們的假說，全部的案例都會判定為錯誤(False)。 小節論 \(c:EnjoySport : X \rightarrow \{ 0, 1 \}\) 是我們的學習目標( Target Concept )。 六個環境狀態( Attributes )： Sky ( 可能的變數有三種 ) \[\{ Sunny, Cloudy, Rainy \}\] Airtamp ( 可能的變數有兩種 ) \[\{ Warm, Cold \}\] Humidity ( 可能的變數有兩種 ) \[\{ Normal, High \}\] Wind ( 可能的變數有兩種 ) \[\{ Strong, Light \}\] Water ( 可能的變數有兩種 ) \[\{ Cold, Warm \}\] Forecast ( 可能的變數有兩種 ) \[\{ Same, Change\}\] 案例空間的大小 \(= 3 \cdot 2 \cdot 2 \cdot 2 \cdot 2 \cdot 2 = 96\) 假說空間的大小 \(= 1 + ( 4 \cdot 3 \cdot 3 \cdot 3 \cdot 3 \cdot 3 ) = 973\) ( 實際上 ) 現在知道目前的假說空間大小後，就要開始找到符合我們期望( Target )的假說( Hypotheses )，那要從何先下手呢？首先，我們可以先從現有的訓練資料使用，其中，我們還可以了解一個概念－－－Inductive learning hypothesis，意思是說，我們今天使用訓練的資訊來找到一個最靠近的假說時，我們也可以找到一些潛在的的規則包含在我們找到的假說之中，其中會有我們從為訓練過的案例( Instance )在內。 ＜注意＞：在實務上來說，有可能訓練的難度會急遽上升，有可能我們的假說空間會超級大，甚至於無限大也有可能，所以要一個個要從所有的假說找到我們需要的是不太可能的，那怎麼辦呢？我們可以利用假說空間的一個特性－－－Partial Ordering，也就是說，這個空間裡的元素，是可以依照一個順序大小排列的。 假說空間的廣至收斂General-to-Specific Ordering over Hypotheses 首先定義幾個名詞，我們有： - 案例( Instance )：\(x\) - 假說( Hypothesis )：\(h\) - 若今天 \(h(x) = 1\) ，稱之 Positive ( True ) Outcome。 由廣至收斂，定義若 \(H_1 \geq_g H_2\)，則可以說 \(H_1\) 比 \(H_2\)還要更廣( General )，舉例： \[ \{ Sunny, ?, ?, ?, ?, ? \} \geq_g \{ Sunny, ?, ?, Strong, ?, ? \} \] GtoSprap Find S Algorithm 將假說 \(h\) 初始化為假說空間 \(H\) 中的最特殊假說 ${ , , , , , } $ 對每個正例 \(x\) ( ＜注意＞我們只使用正例( Positive Outcome )，不用反例！ ) 對 \(h\) 的每個環境狀態( Attribute )進行約束 如果 \(x\) 的該環境狀態滿足 \(h\) 對應的環境狀態，那麼不做任何處理。 否則將 \(h\) 中該環境狀態一般化( Generalize ) 以滿足 \(x\) 的環境狀態。 重複直到所有正例都被尋遍。 輸出最後唯一的假說 \(h\) ，而這個假說正是我們使用訓練資料中的正例所能訓練出最收斂的假說。 findsalgorithm Version Space Definition: Consistent Hypotheses( 認同假說 ) 若有假說 \(h\) 以訓練集合所有的案例進行測試，輸出結果和我們的想法一致，就可以聲明假說 \(h\) 為認同假說( Consistent Hypotheses )。( 下方為原始定義 ) A hypothesis \(h\) is consistent with a set of training examples \(D\) of target concept \(c\) if and only if \(h(x) = c(x)\) for each training example \(&lt;x, c(x)&gt;\) in \(D\). \[ Consistent (h, D) \equiv ( \; \forall &lt;x, c(x ) \in D \;) h(x) = c(x)) \] Definition: Version Space ( 候選空間 ) 候選空間就是對於該測試的資料集，所有的認同假說所組合的空間，因為每個假說都可以符合目前的訓練資料的期望，所以每個假說都等待我們再進一步驗證。( 下方為原始定義 ) The version space \(VS_{H,D}\) , with respect to hypothesis space \(H\) and training examples \(D\), is the subset of hypotheses from \(H\) consistent with all training examples in \(D\). \[ VS_{H,D} \equiv \{ h \in H \; | \; Consistent (h, D) \} \] versionspacegraph Version space for a “rectangle” hypothesis language in two dimensions. Green pluses are positive examples, and red circles are negative examples. GB is the maximally general positive hypothesis boundary, and SB is the maximally specific positive hypothesis boundary. The intermediate (thin) rectangles represent the hypotheses in the version space. Definition: General Boundary General boundary \(G\) of version space \(VS_{H,D}\) : set of most general members Definition: Specific Boundary Specific boundary \(S\) of version space \(VS_{H,D}\) : set of most specific members Version Space Every member of the version space lies between \(S\) and \(G\) \[ VS_{H,D} \equiv \{ h \in H \; | \; (\exists s \in S ) (\exists g \in G ) (g \geq_g h \; \geq_g s) \} \] \[ where \geq_g \equiv more \; general \; than \; or \; equal \; to \] The List-Then-Elimination Algorithm列表消除演算法 起始化： 候選空間( Version Space ) \(\leftarrow(assign)\) 所有在假說空間的假說。 對每個訓練案例\(&lt;x, c(x)&gt;\) 從候選空間消除所有 \(h(x) \neq c(x)\) 的假說 \(h\)。 對所有的訓練案例檢驗過，最後輸出候選空見剩下的假說列表。 優點 保證最後的假說列表裡的所有假說必定和訓練案例的期望相符( Consistent )。 缺點 若今天假說空間是無窮大，那這個方法就不能使用。 要將該假說空間的所有假說窮舉於列表之中。 Candidate-Elimination Algorithm候選消除演算法 ＜前情提要＞：候選空間( Version Space )可以由 Most specific boundaries 與 Most general boundaries 界定出來。 - 正例可以將 Specific boundary 變的更一般化( General )。 Positive examples force specific boundary to become more general. - 反例可以將 General boundary 變的更收斂( Specific )。 Negative examples force general boundary to become more specific. 最後，這個界定出來的假說集合可以符合所有的訓練資料。 In the end, all hypotheses which satisfy training data remain. 起始化 G \(\leftarrow\) 一組在假說空間 \(H\) 最一般化的環境因素。標記為：\(&lt;?, \ldots, ?&gt;\) S \(\leftarrow\) 一組在假說空間 \(H\) 最嚴苛的環境因素。標記為：\(&lt;\phi, \ldots, \phi&gt;\) 對於每個訓練案例 \(d\)，進行以下操作： 若 \(d\) 為一正例： 從 \(G\) 中移除所有與 \(d\) 不一致的假說。 對 \(S\) 中每個對 \(s\) 不一致的假說\(s\)： 將 \(s\) 從 \(S\) 之中移去。 把 \(s\) 的所有的極小泛化假說 \(h\) 加入到S中，其中 \(h\) 滿足 \(h\)與\(d\)一致，且\(G\)的其中一個元素必比起 \(h\) 更泛化。 從\(S\)中移去所有符合這樣的假說：它比S中另一假設更泛化。 若 \(d\) 為一反例： 從 \(S\) 中移去所有 \(d\) 不一致的假說。 對 \(G\) 中每個與 \(d\) 不一致的假設 \(g\)： 從 \(G\) 中移去 \(g\) 。 把 \(g\) 的所有的極小特化式 \(h\) 加入到 \(G\) 中，其中 \(h\) 滿足 \(h\) 與 \(d\) 一致，而且 \(S\) 的某個成員比 \(h\) 更收斂。 從 \(G\) 中移去所有符合這樣的假說：它比 \(G\) 中另一假說更特殊。 範例 - \(EnjoySport\) 起始化 Specific boundary to: \(S_0 = \{(Ø,Ø,Ø,Ø,Ø,Ø)\}\) General boundary to: \(G_0 = \{(?,?,?,?,?,?)\}\) 1’st instance: (Sunny,Warm,Normal,Strong,Warm,Same) = Yes Positive example generalizes Specific boundary \(S_1 = \{ (Sunny,Warm,Normal,Strong,Warm,Same)\}\) \(G_1 = \{ (?,?,?,?,?,?)\}\) 2’nd instance: (Sunny,Warm,High,Strong,Warm,Same) = Yes Positive example generalizes Specific boundary \(S_2 = \{(Sunny,Warm,?,Strong,Warm,Same)\}\) \(G_2 = \{ (?,?,?,?,?,?)\}\) 3’rd instance: (Rainy,Cold,High,Strong,Warm,Change) = No Negative example specializes General boundary \(S_3 = \{ (Sunny,Warm,?,Strong,Warm,Same) \}\) \(G_3 = \{(Sunny,?,?,?,?,?), \quad O.K.\) \((Cloudy,?,?,?,?,?), \quad Not \; more \; general \; than \; S_3\) \((?,Warm,?,?,?,?), \quad O.K.\) \((?,?,Normal,?,?,?), \quad Not \; more \; general \; than \; S_3\) \((?,?,?,Light,?,?), \quad Not \; more \; general \; than \; S_3\) \((?,?,?,?,Cool,?), \quad Not \; more \; general \; than \; S_3\) \((?,?,?,?,?,Same)\} \quad O.K.\) \(\Rightarrow G_3 = \{ (Sunny,?,?,?,?,?), (?,Warm,?,?,?,?), (?,?,?,?,?,Same)\}\) 4’th instance: (Sunny,Warm,High,Strong,Cool,Change) = Yes Positive example generalizes Specific boundary \(S_4 = \{ (Sunny,Warm,?,Strong,?,?) \}\) \(G_4 = \{ (Sunny,?,?,?,?,?), (?,Warm,?,?,?,?) \}\) Final version space is all hypotheses, h such that: \[ g \geq_ g h \; \geq_g s \] candidate-eliminationalgorithm candidate-eliminationalgorithm2 What query should the learner make next? How should these be classified? \[ &lt;Sunny, Warm, Normal, Strong, Cool, Change&gt; \] \[ &lt;Rainy, Cold, Normal, Light, Warm, Same&gt; \] \[ &lt;Sunny, Warm, Normal, Light, Warm, Same&gt; \] Inductive Bias 歸納偏置 ( 未完待補 ) 需要某些的預先設定( 偏見 )。 參考 Mr’ OpenGate - AI - Ch13 機器學習(1), 機器學習簡介與監督式學習 Introduction to Machine Learning, Supervised Learning 《机器学习》第2章中find-s算法的python实现 WEKIPEDIA - Version space learning robert_ai - ML一（概念学习和一般到特殊序）]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Concept Learning</tag>
        <tag>Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[機器學習地圖]]></title>
    <url>%2Fblog%2F2018%2F04%2F20%2F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%9C%B0%E5%9C%96%2F</url>
    <content type="text"><![CDATA[機器學習地圖 類別 監督學習 (Supervised learning)：從給定的訓練數據集中學習出一個模式（函數 / learning model），當新的數據到來時，可以根據這個模式預測結果。監督學習的訓練集要求是包括輸入和輸出，也可以說是特徵和目標。訓練集中的目標是由「人」標註的。常見的監督學習算法包括回歸分析和統計分類( Classify )。 無監督學習 (unsupervised learning)：與監督學習相比，訓練集沒有人為標註的結果。常見的無監督學習算法有聚類( Cluster )。 半監督學習 (Semi-supervised learning)：介於監督學習與無監督學習之間。 增強學習 (reinforcement learning)：通過觀察來學習做成如何的動作。每個動作都會對環境有所影響，學習對象根據觀察到的周圍環境的反饋( 獎勵 )來做出判斷。 機器學習演算法種類 構造條件機率：回歸分析和統計分類 人工神經網絡 決策樹 高斯過程回歸 線性判別分析 最近鄰居法 感知器 徑向基函數核 支持向量機 通過再生模型構造機率密度函數： 最大期望算法 graphical model：包括貝葉斯網和Markov隨機場 Generative Topographic Mapping 近似推斷技術： 馬爾可夫鏈 蒙特卡羅方法 變分法 最優化：大多數以上方法，直接或者間接使用最優化算法。 machinelearningmap kindofmachinelearning 參考 Mr’ OpenGate - AI - Ch13 機器學習(1), 機器學習簡介與監督式學習 Introduction to Machine Learning, Supervised Learning]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Decision Tree Learning]]></title>
    <url>%2Fblog%2F2018%2F04%2F18%2FDecision-Tree-Learning%2F</url>
    <content type="text"><![CDATA[Decision Tree Learning 決策樹學習 簡介 最受歡迎的歸納推理演算法( inductive inference algorithm )。 廣泛且實務的方法。 對於干擾值( Noise )相當敏感。 可用來學習如何以聯集( Disjunctive )表示限制集( Constraints )。 ( Concept Learning 以交集( Conjunctive )表示 ) 呈現的方式相當簡單。 樹狀結構( Tree Structure )、若則表示式( If-Then rules ) \(Ex.\) Play Tennis 範例訓練資料( Training Example ) decisiontreelearning_training example 決策樹( Decision Tree ) decisiontreelearning_exampletree 決策樹( Decision Tree )的介紹 決策樹表示法( Decision Tree Representation ) 每個內節點 Internal node ( 包括根結點 Root node )代表對一個環境狀態( Attribute )檢驗。 而分支( Branch )出來的意義我們可以視為是該環境狀態( Attribute )的一種可能值( Attribute value )。 每個葉節點 Leaf node 給予一個適當的分類結果( Classification )。 我們將每個案例( Instances )分類到一個離散的類別( Categories )之中。 藉由決策樹由根結點至葉節點找到該類別。 決策樹引導的假說( Hypotheses ) 先 AND 再 OR ( 原文：Disjunctions (OR’s) of conjunctions (AND’s) )。 經由根結點往葉節點走可視為是一種對於該環境狀態限制的交集( Conjunction of constraints on attributes )。 而連上兄弟節點( Sibling )的兩個邊( Edges )可視為是一種對於該環境狀態限制的聯集( Separate branches are disjunctions )。 \(Ex \; ( Cont. )\) (Outlook=Sunny and Humidity=Normal) or (Outlook=Overcast) or (Outlook=Rain and Wind=Weak) 注意！ 每個用來訓練的案例 ( Instances )都必須要以「因素-結果」( Attribute - value pairs )的方式給予訓練。 目標訓練函式 ( Target function )的值域是離散的值 ( Discrete value )。 這種方法最後呈現的假說 ( Hypotheses )有可能是一些環境狀態限制( Constraints on attributes )的聯集( Disjunctive )。 極有可能會被不乾淨的資料( Noise )擾亂了學習。 應用於： 醫療或是設備的診斷。 信用額度分析( 銀行 )。 決策樹的種類 世界上有許多有特殊的決策樹演算法( decision-tree algorithms )，比較著名的有： - ID3 (Iterative Dichotomiser 3) - C4.5, C5.0 (successor of ID3) - CART (Classification And Regression Tree) - CHAID (CHi-squared Automatic Interaction Detector). - MARS: extends decision trees to handle numerical data better. 注意 ID3 is the algorithm discussed in textbook.( 在書本中有更詳細的介紹 ) Simple, but representative. ( 簡單但representative? ) Source code publicly available. ( 程式碼是開放的 ) ID3演算法 概述：Top-down, greedy search through space of possible decision trees. ( 在所有可以出現的決策樹中用貪心法由上而下找到較佳的那棵樹。 ) ID3在建構決策樹過程中，以資訊獲利(Information Gain)為準則，並選擇最大的資訊獲利值作為分類屬性。這個算法是建立在奧卡姆剃刀的基礎上：越是小型的決策樹越優於大的決策樹。儘管如此，該算法也不是總是生成最小的樹形結構，而是一個啟發式算法。另外，C4.5算法是ID3的升級版。 Decision trees represent hypotheses, so this is a search through hypothesis space. ( 決策樹亦也代表是一種假說，所以這個演算法也可以說是在所有的假說中找到一個較佳的假說。 ) 那個演算法該如何起手呢？ 決定甚麼環境因素( Attribute )應該放在根結點( Root node )？ 接著由上而下( Top-down )的建構決策樹，對每個後繼的節點( Successive node )使出一樣的決策手段選出該節點應該置入何種環境因素( Attribute )。 注意！千萬不要由下往上參考之前選過的值，因為我們以貪心法則，所以目前的最佳解決不可能出現在之前選過的環境因素( Attribute )之中，或是受其干擾。 ( Never backtracks to reconsider earlier choices. ) 同上述，在每次的選擇之中，由於我們認知這種情況適用貪心法( Greedy Method )，所以我們每次環境因素( Attribute )的選擇都朝向我們最後最佳的假說靠近。 虛擬碼( Pseudo Code ) 12341. 使用屬性計算與之相關的樣本熵值2. 選取其中熵值最小的屬性(資訊獲利最大)3. 生成包含該屬性的節點4. 遞迴直到終止 decisiontreelearning_builddecisiontree_algorithm decisiontreelearning_howtochoosenode ＜討論＞ ID3演算法的終極目標，就是要將決策樹中每個節點都擺上最優的環境因素( Attributes )。 \(Question.\) 到底以甚麼條件決定甚麼因素要擺放於哪個節點？ \(Answer.\) 資訊獎賞 or 資訊獲利( Information gain )。 - 資訊獲利( Information gain ) 統計該價值以檢視該環境因素置於何處來分類我們的資料，我們使用熵( entropy 又稱“亂度” )來定義這邊的資訊獲利( Information gain )。( 原文：Statistical quantity measuring how well an attribute classifies the data. Use entropy to define information gain. ) ID3 和 C4.5 - Information gain ( 資訊獲利 ) 與 Gain ratio 定義 關心其中一個環境因素( Attribute )\(A\) 的資訊獲利( Information gain )我們標記為 \(Gain( S, A )\)，且我們關心的目標樣本群體為 \(S\)，其中： \[Gain( S, A ) = Entropy( S ) - \sum_{ v \in Values(A) } ( \frac{S_v}{S}Entropy(S_v) )\] - \(v\) ranges over values of \(A\) - \(S_v\): members of \(S\) with \(A = v\) - \(1^{st}\) term: the entropy of \(S\) - \(2^{nd}\) term: expected value of entropy after partitioning with \(A\) Example： PlayTennies 四個環境變因 Outlook = {Sunny, Overcast, Rain} Temperature = {Hot, Mild, Cool} Humidity = {High, Normal} Wind = {Weak, Strong} 欲看討的結果 - 開心或是不開心( Target Attributes - Binary ) PlayTennis = {Yes, No} 今天有14組訓練資料 9筆的結果是開心的 ( Positive ) 5筆的結果是不開心的( Negative ) 訓練資料表 decisiontreelearning_trainningdataform Step 1. 計算整體的亂度( Entropy ) \(N_\oplus = 9, N_\ominus = 5, N_{Total} = 14\) \(Entropy( S ) = -\frac{9}{14} \cdot \lg (\frac{9}{14}) - \frac{5}{14} \cdot \lg ( \frac{5}{14} ) = 0.940\) Step2. 不斷計算資訊獲利( 找亂度比較低attribute的 )，選擇最大值當作根結點 Outlook Outlook = Sunny \[N_\oplus = 2, N_\ominus = 3, N_{Sunny} = 5\] \[Entropy(S_{Sunny}) = -(\frac{2}{5})\cdot \log_2(\frac{2}{5}) - (\frac{3}{5}) \cdot \log_2(\frac{3}{5}) = 0.971\] Outlook = Overcast \[N_\oplus = 4, N_\ominus = 0, N_{Overcast} = 4\] \[Entropy(S_{Overcast}) = -(\frac{4}{4})\cdot \log_2(\frac{4}{4}) - (\frac{0}{4}) \cdot \log_2(\frac{0}{4}) = 0.0\] Outlook = Rain \[N_\oplus = 3, N_\ominus = 2, N_{Rain} = 5\] \[Entropy(S_{Rain}) = -(\frac{3}{5})\cdot \log_2(\frac{3}{5}) - (\frac{2}{5}) \cdot \log_2(\frac{2}{5}) = 0.971\] 計算環境因素的 Outlook 之資訊獲利 \[Gain(S, Outlook) = Entropy(S) - (N_{Sunny} / N_{total}) * Entropy(S_{Sunny})\] \[ - (N_{Overcast} / N_{total}) * Entropy(S_{Overcast})\] \[ - (N_{Rain} / N_{total} ) * Entropy(S_{Rain})\] \[\Rightarrow 0.940 - (5/14) \cdot 0.971 - (4/14) \cdot 0.00 - (5/14) \cdot 0.971 = 0.246\] Temperature Repeat process over { Hot, Mild, Cool } \[ Gain( S, Temperature ) = 0.029 \] Humidity Repeat process over { High, Normal } \[ Gain( S, Humidity ) = 0.151 \] Wind Repeat process over { Weak, Strong } \[ Gain( S, Wind ) = 0.048 \] 再來，我們要找到最佳的資訊獲利( Information gain )，其中： \[Gain(S, Outlook) = 0.246\] \[ Gain( S, Temperature ) = 0.029 \] \[ Gain( S, Humidity ) = 0.151 \] \[ Gain( S, Wind ) = 0.048 \] 從亂度的點看來，似乎Outlook的亂度最低( 與宇亂度相減後剩餘比較多資訊獲利 )，所以我們選擇Outlook作為我們根結點( root node )，如下圖： decisiontreelearning_choosenode 選擇了Outlook做為決策樹的根結點後，緊接著，我們可以將三種不同的Outlook作為分支，其中特別的是，Overcast狀態之中( 上圖中間綠色部分 )，全部皆為開心狀態( Positive outcome )，所以可以直接決定Overcast輸出為開心( Positive )。 Step 2. Conti. - 選擇下一個節點( 子樹的根結點 ) ( 從何子節點開始建子樹？ I don’t know yet. ) Same steps as earlier but only examples sorted to the node are used in Gain computations.( 無法理解 ) 選一個點( 隨機？ )繼續建子樹 Outlook = Sunny \[Gain(S_{Sunny}, Humidity) = 0.97 - (3/5) \cdot 0 - (2/5) \cdot 0 = 0.97 bits\] \[Gain(S_{Sunny}, Temperature) = 0.97 - (2/5) \cdot 0 - (2/5) \cdot 1 - (1/5) \cdot 0 = 0.57 bits\] \[Gain(S_{Sunny}, Wind) = 0.97 - (2/5) \cdot 1 - (3/5) \cdot 0.918 = 0.019 bits\] 由上式可以看出來Humidity的亂度最小，所以選擇之為此子樹的根。 Final Decision Tree decisiontreelearning_finaldecisiontree 熵、亂度 (Entropy) 介紹 在資訊理論中，熵被用來衡量一個隨機變數出現的期望值(機率與統計)。它代表了在被接收之前，訊號傳輸過程中損失的資訊量，又被稱為資訊熵。熵是對不確定性的測量。在資訊界，熵越高則能傳輸越多的資訊( 資訊越多意味著有更多的可能性 )，熵越低則意味著傳輸的資訊越少( 資訊越少意味著有更少的可能性 )。 如果有一枚理想的硬幣，其出現正面和反面的機會相等，則拋硬幣事件的熵等於其能夠達到的最大值。我們無法知道下一個硬幣拋擲的結果是什麼，因此每一次拋硬幣都是不可預測的。( 越是不可預測的結果 \(\rightarrow\) 亂度越大，而這種結果，正是造成人類選擇障礙的原因，所以我們希望熵越低越好，我們可以立即做出判斷 ) \(Ex1.\) 使用一枚正常硬幣進行拋擲，這個事件的熵是一位元，若進行n次獨立實驗，則熵為\(n\)，因為可以用長度為 \(n\) 的位元流表示。但是如果一枚硬幣的兩面完全相同，那個這個系列拋硬幣事件的熵等於零，因為結果能被準確預測。 \(Ex2.\) \(Let \; y \; be \; a \; Boolean \; function, and \; let \; P \; denote \; Probability.\) What is the most pure (亂度低) probability distribution? \[P(y = 0) = 1, P(y = 1) = 0\] \[P(y = 0) = 0, P(y = 1) = 1\] What is the most impure (亂度高) probability distribution? \[P(y = 0) = 0.5, P(y = 1) = 0.5\] 意同於最大的亂度。 定義 首先，我們可以先從簡單的看討當目前的結果最多只有兩種情況，如拋硬幣，最多只有正面或是反面，下圖\(x\)軸\(P_\oplus\)代表擲出正面的機率函數，而\(y\)軸則是對應的熵值，而\(P_\ominus\)的機率軸則是會隨著\(P_\oplus\)下降而上升( 兩者互補 )，但是對應到的熵值會一樣大。 decisiontreelearning_entropygraph \(S\) is a sample of training examples( 隨機變量 ). 當今天的結果只有正與反 ( 與硬幣一樣 )時，觀察目前的隨機變量 - 我們令： - \(P_\oplus\) ( 就目前隨機變數產生的機率 ) is the portion of the positive examples ( 正面 ) in \(S\). - \(P_\ominus\) ( 就目前隨機變數產生的機率 ) is the portion of the negative examples ( 反面 ) in \(S\). Entropy ( 熵 ) measures the impurity ( 亂度 ) of \(S\). 我們先定義熵值 ( Entropy ) 如下： \[ Entropy( S ) = E( I( S ) ) = E(- \ln ( P ( S ) ) ) \] 其中，\(E\)為期望函數，\(I( S )\)是 \(S\) 的資訊量（又稱為資訊本體），\(I( S )\)也是一個隨機變數。 所以在取硬幣的樣本( \(S\) )完後，我們可以將熵值寫成： \[ Entropy( S ) = \sum_{i = 1}^{2} P(S_i)I(S_i)\] \[ \Rightarrow -\sum_{i = 1}^{2} P(S_i)\log_{2} P(S_i) \] \[ \Rightarrow -P_{\oplus}\log_2 P_{\oplus} - P_{\ominus}\log_2 P_{\ominus}\] ＜Note＞ \[\sum_{i = 1}^N P_i = 1 \; and \; 0 \leq P_i \leq 1 \] 推廣至一般式 decisiontreelearning_generalentropygraph 當取自有限的樣本時，熵的公式可以表示為： \[H(X) = \sum _{i} P(x_i) \, I(x_i)=-\sum_i P(x_i)\log _b P(x_i)\] 在這裏 \(b\) 通常是\(2\),自然常數 \(e\)，或是\(10\)。當\(b = 2\)，熵的單位是\(bit\)；當\(b = e\)，熵的單位是\(nat\)；而當\(b = 10\),熵的單位是\(Hart\)。 ＜Note＞ 定義當\(P_i = 0\)時，對於一些 \(i\) 值，對應的被加數 \(0 \log_b 0\) 的值將會是 \(0\)，這與極限一致。 \[ \Rightarrow \lim_{p\to0+} ( p\log p ) = 0 \] CART (Classification and Regression Tree) 見 Mr’ opengate - AI - Ch14 機器學習(2), 決策樹 Decision Tree 決策樹學習的常見問題 避免過度適配資料( Prevent Overfitting ) 首先，相較於很冗長的樹，在機器學習中其實比較偏向於比較矮的樹，然而，為何？我們可以由Occam’s Razor ( 奧坎剃刀 )得知，若有兩個假說同時都能解釋該現象，我們偏向於比較沒那麼嚴個的假說( 可以表達比較廣的概念 )。過度配適是指模型對於範例的過度訓練，導致模型記住的不是訓練資料的一般特性，反而是訓練資料的局部特性。對測試樣本的分類將會變得很不精確。 ＜注意＞通常過度適配發生在訓練範例含有雜訊和離異值時，但當訓練數據沒有雜訊時，過度適配也有可能發生，特別是當訓練範例的數量太少，使得某一些屬性「恰巧」可以很好地分割目前的訓練範例，但卻與實際的狀況並無太多關係。 decisiontreelearning_overfitting1 decisiontreelearning_overfitting2 解決方案：修剪決策樹移除不可信賴的分支 事前修剪 (Prepruning) : 透過決策樹不再增長的方式來達到修剪的目的。選擇一個合適的臨界值往往很困難。 事後修剪 (Postpruning) : 子樹置換 (Subtree Replacement)：選擇某個子樹，並用單個樹葉來置換它。 子樹提升 (Subtree Raising)： decisiontreelearning_subtreeraising 合併連續值屬性 透過動態地定義新的離散值屬性來實現，即先把連續值屬性的值域分割為離散的區間集合，或設定門檻值以進行二分法。 屬性選擇指標的其他度量標準 訊息獲利 : 趨向於包含多個值的屬性 獲利比率 : 會產生不平均的分割，也就是分割的一邊會非常小於另一邊 吉尼係數 : 傾向於包含多個值的屬性，當類別個數很多時會有困難，傾向那些會導致平衡切割並且兩邊均為純粹的測試 ＜尚有其他的度量標準，也都各有利弊＞ 例題 A data set has 4 Boolean variables. What is the maximum number of leaves in a decision tree? \(2^4\) To each leaf in the decision, the number of corresponding rule is 1 If a decision tree achieves 100% accuracy on the training set, then it will also get 100% accuracy on the test set? No Using information gain to pick attributes, decision tree learning can be considered A* search algorithm. No A decision tree can describe any Boolean function? Yes 補充 C4.5 演算法 C4.5演算法利用屬性的獲利比率(Gain Ratio)克服問題，獲利比率是資訊獲利正規化後的結果。求算某屬性A的獲利比率時除資訊獲 利外，尚需計算該屬性的分割資訊值(Split Information) : SplitInfoA(S)=∑t∈T|Sj||S|×log2|Sj||S| C4.5的改善：對連續屬性的處理 改善ID3傾向選擇擁有許多不同數值但不具意義的屬性：之所以使用獲利比率(Gain Ratio)，是因為ID3演算法所使用的資訊獲利會傾向選擇擁有許多不同數值的屬性，例如：若依學生學號(獨一無二的屬性)進行分割，會產生出許多分支，且每一個分支都是很單一的結果，其資訊獲利會最大。但這個屬性對於建立決策樹是沒有意義的。 C5.0 演算法 C5.0 是 C4.5的商業改進版，可應用於海量資料集合上之分類。主要在執行準確度和記憶體耗用方面做了改進。因其採用Boosting方式來提高模型準確率，且佔用系統資源與記憶體較少，所以計算速度較快。其所使用的演算法沒有被公開。 C5.0 的優點： C5.0模型在面對遺漏值時非常穩定。 C5.0模型不需要很長的訓練次數。 C5.0模型比較其他類型的模型易於理解。 C5.0的增強技術提高分類的精度。 參考 Mr’ opengate - AI - Ch14 機器學習(2), 決策樹 Decision Tree Wiki - 熵 - 資訊理論 奧坎剃刀]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Decision Tree Lrearning</tag>
        <tag>Data Mining</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有關字串的名詞解釋]]></title>
    <url>%2Fblog%2F2018%2F03%2F21%2F%E6%9C%89%E9%97%9C%E5%AD%97%E4%B8%B2%E7%9A%84%E5%90%8D%E8%A9%9E%E8%A7%A3%E9%87%8B%2F</url>
    <content type="text"><![CDATA[字元 (character)： 孤單的一個符號。’7’,’1’, ’ 阿’, ’2’, ’ a’, ’ X’ 2. 字元集 (Alphabet)： 由字元組成的集合，通常會用\(\sum\)表示。 3. 字串 (String)： 由字元集中的字元構成的序列。”7122” 4. 子字串 (Substring)： 字串中的一段連續字元。”71” in ”7122” 5. 子序列 (Subsequence)： 字串中不需連續的一斷字元。”72” in ”7122” 6. 前綴 (Prefix)： 一個子字串包含第一個字元。”7”, ”71”, ”712”, ”7122” in ”7122”，在這裡所有文章我會命名為前總和，方便閱讀。 7. 後綴 (Suffix)： 一個子字串包含最後一個字元。”2”, ”22”, ”122”, ”7122” in ”7122”，，在這裡所有文章我會命名為後總和，方便閱讀。 8. 字典序 (Alphabetical Order)： 定義字串間的大小。先定義字元間的大小：\[’\,’ &lt; ’a’ &lt; ’b’ &lt; ’c’ &lt; ’d’ &lt; …&lt; ’z’\]通常就是照著 ASCII 碼的編排順序，要注意的是 空字元 比其他字元都小 接下來從第一個位置一位一位比對，由左而右比對方小的就是比較小的字串。 9. 後綴數組 (Suffix Array)： 將一個字串的所有後綴(後總和) ，照字典序排序後，所得的名次陣列。\[Sa[i]: 第i個後綴\] 10. 排名數組 (Rank Array)： 為後綴數組的逆數組。\[Ra[i]: 第 i 個後綴是*第幾名*\] 11. 最長共同前綴 (Longest Common Prefix)： 兩個字串，從第一位一位一位比對，直到不一樣就停止 \(ex:\) ’712221212’ 和’712222222’ 的LCP(最長共同前綴)：’71222’。 12. lcp(I, J)： 對於一個字串，他的第 I 個後綴和第 J 個後綴的 LCP 有多長 13. LCP(I, J)： 對於一個字串，他的第 I 名後綴與第 J 名後綴的 LCP 有多長 14. height[i]： 對於一個字串，LCP(i − 1, i) 15. h[i]： 對於一個字串，LCP(Ra[i] − 1, Ra[i]) # 參考 建國中學 2012 年資訊能力競賽培訓講義 - 08]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Z - algorithm]]></title>
    <url>%2Fblog%2F2018%2F03%2F19%2FZ-algorithm%2F</url>
    <content type="text"><![CDATA[這個演算法可以線性時間在一段文本(text) 裡面找到所有我們欲求的段落(pattern)。 今天，當我們的文本(text)的長度為 \(n\) 且欲求的段落(pattern)為 \(m\)時 ，搜尋只需要線性長度的時間 \(O(m+n)\) 即可，雖然這個演算法需要的空間(space complexity)與時間複雜度(time complexity)都與KMP algorithm一致，但是這個演算法比起KMP algoritjm還要容易了解。 KMP algorithm：每個前綴與其後綴的次長共同前綴（最長的後綴） Z algorithm：每個後綴與母字串的最長共同前綴（單純的長度） 首先，我們需要一個 \(Z\)陣列(\(Z\) array) \(Z\)陣列 當我們將欲檢索的文本存為一個字串 $ str[0 \ldots n-1] $ 時，同時也建立一個與字串一樣長的\(Z\)陣列。 在\(Z\)陣列中，第 \(i\) 元素紀錄「最長共同前總和 (Longest Common Prefix)的長度」，而 LCP 的長度 是由「從 \(i\) 開始的後總和 (Postfix)」與「該文本」共同決定。 ( 注意： \(Z[0]\) 毫無意義可言，因為從第0個開始的後總和(Postfix) 必與原本的文本字串相同。 ) 大致上我們可以看成如下的函式： Z演算法的表示法 \(Ex.\) 123Index 0 1 2 3 4 5 6 7 8 9 10 11 Text a a b c a a b x a a a zZ values 1 0 0 3 1 0 0 2 2 1 0 \(More\) \(ex.\) 12345678str = &quot;aaaaaa&quot;Z[] = &#123;x, 5, 4, 3, 2, 1&#125;str = &quot;aabaacd&quot;Z[] = &#123;x, 1, 0, 2, 1, 0, 0&#125;str = &quot;abababab&quot;Z[] = &#123;x, 0, 6, 0, 4, 0, 2, 0&#125; \(Z\) 陣列如何幫助演算法加速? 這個演算法的想法是將段落(pattern)與文本字串(text string)連接起來，若視段落(pattern)為「P」，視文本字串(text string)為「T」，並加上一個從未在段落與文本中出現過的字元「\$」再產生出如「P$T」的字串。 最後，我們再產生一個屬於「P$T」的 Z陣列，在 Z陣列之中，若該 Z值等於段落(pattern)的長度，段落出現在該處。 12345678910Example:Pattern P = &quot;aab&quot;, Text T = &quot;baabaa&quot;The concatenated string is&quot;a, a, b, $, b, a, a ,b ,a, a&quot;.Z array for above concatenated string is &#123;x, 1, 0, 0, 0, 3, 1, 0, 2, 1&#125;. ^Since length of pattern is 3, the value 3 in Z array indicates presence of pattern. 如何建立 \(Z\)陣列 最簡單的就是使用兩個迴圈，外層迴圈將整個「P$T」跑過一遍，內層迴圈則是看看到底 i 位置的後總和與「P$T」的LCP長度為何。 \(Time\) \(complexity:\) \[O(n^2)\] 我們當然可以使用另一種方法讓建立陣列的時間複雜度降低。 此演算法的關鍵在於要維護一個區間\([L \ldots R]\)，\(R\) 的位置代表由 \(L\) 處之後可以和整個字串最長的前總和重疊到的最後一個位置( 換句話說：\([L \ldots R]\)是整個字串的前綴子字串 )，若完全不重疊，則 \(L\) 與 \(R\)相等。 步驟 (\(i\) 為當前位置) 若 \(i &gt; R\) ，就代表當前 \(i\) 沒有經過任何「P$S」的前綴子字串，所以重置 \(L\) 與 \(R\) 的位置(\(L = i, R = i\))，經由比對「P$S」的前綴與 \(i\) 之後的前綴，並找出最長的子字串(\(R\) 的位置)，計算新的 \(L\) 與 \(R\) 的位置，也一併將 \(Z[i]\)值算出來(\(= R - L + 1\))。 若 \(i \leq R\) ，令 \(K = i - L\) ，再來我們知道 \(Z[i] \geq min(Z[K], R-i+1)\) 因為\(String[i \ldots]\)與\(String[K\ldots]\)共同前\(R-i+1\)個字元必然為[P$T]的前綴子字串。現在有兩種情形會發生： case1： 若\(Z[K] &lt; R-i+1\) ，代表沒有任何「P$S」的前綴子字串 從 \(i\) 位置開始(否則 \(Z[K]\) 的值會更大)，所以也意味著\(Z[i] = Z[K]\)，還有區間\([L\ldots R]\)不變。 case2： 若\(Z[K] \geq R-i+1\)，代表\(String[i \ldots]\)可以和\(String[0\ldots]\) 繼續比對相同的字元，也就意味有可能拓展\([L \ldots R]\) 區間，因此，我們會設 \(L = i\) ，接著從 \(R\) 之後開始繼續比對「P$S」的前綴子字串，最後我們會得到新的\(R\)，並更新\([L \ldots R]\) 區間與計算 \(Z[i]\) \(( = R - L + 1)\)。 想要了解上述的演算法可以經由這個連結觀看動畫。 小視窗 Z演算法的子問題 如果一個位置 \(i\) 位於之前比過的那段 \([L, R]\) 當中，他是否跟 \(Z[i − L]\) 相同呢？我們可以分成三種情形： 1. 要比的後綴根本不在以前比過的範圍\([L, R]\)內 → 就去比吧！ 2. 要比的後綴在以前比過的範圍\([L, R]\)但長度未知 → 還是去比吧！ 3. 要比的後綴在以前比過的範圍\([L, R]\)但長度已知 → 直接記錄囉！ 程式碼實作 台大資工PPT by nkng 12345678910void z_build(const char *S, int *Z) &#123; Z[0] = 0; int bst = 0; for(int i = 1; S[i]; i++) &#123; if(Z[bst] + bst &lt; i) Z[i] = 0; else Z[i] = min(Z[bst]+bst-i, Z[i-bst]); while(S[Z[i]] == S[i+Z[i]]) Z[i]++; if(Z[i] + i &gt; Z[bst] + bst) bst = i; &#125;&#125; Z algorithm - GeeksforGeeks 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// A C++ program that implements Z algorithm for pattern searching#include&lt;iostream&gt;using namespace std; void getZarr(string str, int Z[]); // prints all occurrences of pattern in text using Z algovoid search(string text, string pattern)&#123; // Create concatenated string "P$T" string concat = pattern + "$" + text; int l = concat.length(); // Construct Z array int Z[l]; getZarr(concat, Z); // now looping through Z array for matching condition for (int i = 0; i &lt; l; ++i) &#123; // if Z[i] (matched region) is equal to pattern // length we got the pattern if (Z[i] == pattern.length()) cout &lt;&lt; "Pattern found at index " &lt;&lt; i - pattern.length() -1 &lt;&lt; endl; &#125;&#125; // Fills Z array for given string str[]void getZarr(string str, int Z[])&#123; int n = str.length(); int L, R, k; // [L,R] make a window which matches with prefix of s L = R = 0; for (int i = 1; i &lt; n; ++i) &#123; // if i&gt;R nothing matches so we will calculate. // Z[i] using naive way. if (i &gt; R) &#123; L = R = i; // R-L = 0 in starting, so it will start // checking from 0'th index. For example, // for "ababab" and i = 1, the value of R // remains 0 and Z[i] becomes 0. For string // "aaaaaa" and i = 1, Z[i] and R become 5 while (R&lt;n &amp;&amp; str[R-L] == str[R]) R++; Z[i] = R-L; R--; &#125; else &#123; // k = i-L so k corresponds to number which // matches in [L,R] interval. k = i-L; // if Z[k] is less than remaining interval // then Z[i] will be equal to Z[k]. // For example, str = "ababab", i = 3, R = 5 // and L = 2 if (Z[k] &lt; R-i+1) Z[i] = Z[k]; // For example str = "aaaaaa" and i = 2, R is 5, // L is 0 else &#123; // else start from R and check manually L = i; while (R&lt;n &amp;&amp; str[R-L] == str[R]) R++; Z[i] = R-L; R--; &#125; &#125; &#125;&#125; // Driver programint main()&#123; string text = "GEEKS FOR GEEKS"; string pattern = "GEEK"; search(text, pattern); return 0;&#125; 建國中學 2012 年資訊能力競賽培訓講義 - 08 123456789101112void Z_maker( int z[], char s[], int n )&#123; z[0] = n; int L = 0, R = 0, i, x; for( i = 1 ; i &lt; n ; i++ )&#123; if( R &lt; i || z[i-L] &gt;= R-i+1 )&#123; R &lt; i ? x = i : x = R+1; while( x &lt; n &amp;&amp; s[x] == s[x-i] ) x++; z[i] = x-i; if( i &lt; x )&#123; L = i; R = x-1; &#125; &#125; else z[i] = z[i-L]; &#125;&#125; Z algorithm - codeforces 12345678910111213141516int L = 0, R = 0;for (int i = 1; i &lt; n; i++) &#123; if (i &gt; R) &#123; L = R = i; while (R &lt; n &amp;&amp; s[R-L] == s[R]) R++; z[i] = R-L; R--; &#125; else &#123; int k = i-L; if (z[k] &lt; R-i+1) z[i] = z[k]; else &#123; L = i; while (R &lt; n &amp;&amp; s[R-L] == s[R]) R++; z[i] = R-L; R--; &#125; &#125;&#125; Z algorithm1 - 日月卦長的模板庫 123456789inline void z_alg1(char *s,int len,int *z)&#123; int l=0,r=0; z[0]=len; for(int i=1;i&lt;len;++i)&#123; z[i]=r&gt;i?min(r-i+1,z[z[l]-(r-i+1)]):0; while(i+z[i]&lt;len&amp;&amp;s[z[i]]==s[i+z[i]])++z[i]; if(i+z[i]-1&gt;r)r=i+z[i]-1,l=i; &#125;&#125; Z algorithm2 - 日月卦長的模板庫 123456789inline void z_alg2(char *s,int len,int *z)&#123; int l=0,r=0; z[0]=len; for(int i=1;i&lt;len;++i)&#123; z[i]=i&gt;r?0:(i-l+z[i-l]&lt;z[l]?z[i-l]:r-i+1); while(i+z[i]&lt;len&amp;&amp;s[i+z[i]]==s[z[i]])++z[i]; if(i+z[i]-1&gt;r)r=i+z[i]-1,l=i; &#125;&#125; 培訓-4 字串- tioj 12345678910void z_build(const char* S,int *z)&#123; z[0]=0; int bst=0; for(int i=1;S[i];i++)&#123; if(z[bst]+bst&lt;i) z[i]=0; else z[i]=std::min(z[bst]+bst−i,z[i−bst]); while(S[z[i]]==S[i+z[i]]) z[i]++; if(z[i]+i&gt;z[bst]+bst) bst=i; &#125;&#125; 例題 TIOJ 1725_Z algorithm_Massacre at Camp Happy 參考 Z algorithm - GeeksforGeeks 建國中學 2012 年資訊能力競賽培訓講義 - 08 培訓-4 字串- tioj 台大資工講義 by nkng Z algorithm - codeforces Gusfield algorithm - momo funny codes Z algorithm - 日月卦長的模板庫 待補充 KMP 字串比對演算法 http://mropengate.blogspot.tw/2016/01/leetcode-kmpimplement-strstr.html]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>Substring</tag>
        <tag>Z Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[泰勒級數]]></title>
    <url>%2Fblog%2F2018%2F03%2F16%2F%E6%B3%B0%E5%8B%92%E7%B4%9A%E6%95%B8%2F</url>
    <content type="text"><![CDATA[在數學中，泰勒級數（英語：Taylor series）用無限項連加式——級數來表示一個函數，這些相加的項由函數在某一點的導數求得。泰勒級數是以於1715年發表了泰勒公式的英國數學家布魯克·泰勒（Sir Brook Taylor）來命名的。通過函數在自變量零點的導數求得的泰勒級數又叫做麥克勞林級數，以蘇格蘭數學家科林·麥克勞林的名字命名。 拉格朗日在1797年之前，最先提出帶有餘項的現在形式的泰勒定理。實際應用中，泰勒級數需要截斷，只取有限項，可以用泰勒定理估算這種近似的誤差。一個函數的有限項的泰勒級數叫做泰勒多項式。一個函數的泰勒級數是其泰勒多項式的極限（如果存在極限）。即使泰勒級數在每點都收斂，函數與其泰勒級數也可能不相等。開區間（或複平面開片）上，與自身泰勒級數相等的函數稱為解析函數。 定義 在數學上，一個在實數或複數 \(a\) 在 鄰域上的無窮可微實變函數或複變函數 \(f(x)\) 的泰勒級數是如下的冪級數 (若與原函式相等時為解析函數)： \[ f(x) \simeq \sum_{n=0}^{\infty }{\frac {f^{(n)}(a)}{n!}}(x-a)^{n} \] 而 \(f^{(n)}(a)\)表示函數 \(f\) 在點 $ a$ 處的 $ n$ 階導數。如果 \(a=0\) ，那麼這個級數也可以被稱為麥克勞林級數。 而多項式函數 \(f(x)\) 在 \(x = a\) 時，\(n\) 階的泰勒展開式 \(P_{n}(x)\) 是： \[ P_{n}(x) = \sum_{i = 0}^{n} \frac{ f^{(i)}(a) }{ i! }\cdot \left(x-a \right)^{i} \] 解析函數 Read more-1 (wiki)，Read more-2 (wiki) 如果泰勒級數對於區間 \((a-r,a+r)\)中的所有 \(x\) 都收斂並且級數的和等於 \(f(x)\) ，那麼我們就稱函數 \(f(x)\) 為解析的（analytic）。若且唯若一個函數可以表示成為冪級數的形式時，它才是解析的。為了檢查級數是否收斂於 \(f(x)\)，通常採用泰勒定理估計級數的餘項 (數值方法)。上面給出的冪級數展開式中的係數正好是泰勒級數中的係數。 泰勒級數的重要性體現在以下三個方面： 冪級數的求導和積分可以逐項進行，因此求和函數相對比較容易。 一個解析函數可被延伸為一個定義在複平面上的一個開片上的解析函數，並使得複分析這種手法可行。 泰勒級數可以用來近似計算函數的值。 對定值 x 而言，函數的精準度會隨著多項式的次數 n 的增加而增加。 對一個固定次數的多項式而言， 確度隨著 x 離開 x=0 處而遞減。 泰勒級數列表(常用) 注意：核函數 $ x$ 為 複數 時它們依然成立！ 幾何級數(等比數列) \[ \frac {1}{1-x} = \sum_{n=0}^{\infty }x^{n}\quad \forall x:\left|x\right|&lt;1 \] 二項式定理 \[ (1+x)^{\alpha }=\sum_{n=0}^{\infty }C^\alpha_n \cdot x^{n}\quad \forall x:\left|x\right|&lt;1,\forall \alpha \in \mathbb{C} \] 指數函數 \[ e^{x}=\sum_{n=0}^{\infty }{\frac {x^{n}}{n!}}\quad \forall x \] \(f(x) = e^x\) 在 \(x = 0\) 的泰勒展開式。 當\(n = 1\)時，\(P_{1}(x) = 1+ \frac{\left( e^0\right)&#39;}{1!}\cdot\left( x - 0 \right)^1\) 當\(n = 2\)時，\(P_{2}(x) = 1+ \frac{\left( e^0\right)&#39;}{1!}\cdot\left( x - 0 \right)^1 + \frac{\left( e^0\right)&#39;&#39;}{2!}\cdot\left( x - 0 \right)^2\) 當\(n = 3\)時，\(P_{3}(x) = 1+ \frac{\left( e^0\right)&#39;}{1!}\cdot\left( x - 0 \right)^1 + \frac{\left( e^0\right)&#39;&#39;}{2!}\cdot\left( x - 0 \right)^2 + \frac{\left( e^0\right)^{(3)}}{3!}\cdot\left( x - 0 \right)^3\) \(...\) 自然對數 \[ \ln(1+x)=\sum_{n=1}^{\infty }{\frac {(-1)^{n+1}}{n}}x^{n}\quad \forall x\in (-1,1] \] 牛頓插值公式的淵源 Read more-1(wiki)，Read more-2(wiki) 牛頓插值公式也叫做牛頓級數，由「牛頓 前向 差分方程」的項組成，得名於伊薩克·牛頓爵士。一般稱其為連續「泰勒展開」的離散對應。 差分 差分，又名差分函數或差分運算，是數學中的一個概念。它將原函數 \(f(x)\) 映射到 \(f(x+a)-f(x+b)\) 。差分運算，相應於微分運算，是微積分中重要的一個概念。 定義 前向差分的定義為： \[ \Delta_{h}^{1}[f](x) = f(x + h) - f(x) \] \[ \Delta_{h}^{n}[f](x) = \Delta_{h}^{n-1}[f](x + h) - \Delta_{h}^{n-1}[f](x) \] $, where $ $ h =$ $ “x”$ \(一步的間距，若無下標h，那間距h = 1。\) 前向差分 函數的前向差分通常簡稱為函數的差分。對於函數 \(f(x)\) ，如果在等距節點： \[ x_{k}=x_{0}+kh,(k=0,1,...,n) \] \[ \Delta f(x_{k})=f(x_{k+1})-f(x_{k}) \] 則稱 \(\Delta f(x)\)，函數在每個小區間上的增量 \(y_{k+1}-y_{k}\) 為 \(f(x)\) 一階差分。 後向差分 對於函數 \(f(x_{k})\)，如果： \[ \nabla f(x_{k})=f(x_{k})-f(x_{k-1}) \] 則稱 \(\nabla f(x_{k})\) 為 \(f(x)\) 的一階逆向差分。]]></content>
      <categories>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Taylor Series</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指對數]]></title>
    <url>%2Fblog%2F2018%2F03%2F15%2F%E6%8C%87%E5%B0%8D%E6%95%B8%2F</url>
    <content type="text"><![CDATA[“\(e^{x}\)” 緣起 首先，要先從複利的公式開始說明： \[ 一年後的本利和 = \left(1+\frac{年利率}{期數} \right)^{期數} \] 其中期數就是看多久複利一次。一個月複利一次的話期數就是12。 依上述所說，設 1 份借貸有 x 年利率，逐月複利話，則每月增加當前值的 \(\frac{x}{12}\) 倍，每月總值都要乘以 \(1 + \frac{x}{12}\)，一年的總值為 \(\left(1 + \frac{x}{12} \right)^{12}\)，逐日複利的話，就是 \(\left (1+ \frac{x}{365} \right)^{365}\)。設年中時段數可為無限，則有如下最初由歐拉提出的指數函數定義： \[ \\lim_{n \to \infty}\left ( 1 + \frac{x}{n} \right )^{n} \] 然後才真正的導出e這個數字，一開始存1元，如果年利率是100%，如果每分每秒都算利息，那麼一年後會得到的利息大約是2.71828，如果把算利息的區間縮到無限小，也就是期數變成無限大的話，就會得到 \[ \lim_{n \to \infty}\left ( 1 + \frac{1}{n} \right )^{n} = e \] 由上式我們則可以得到, \[ \lim_{n \to \infty}\left ( 1 + \frac{x}{n} \right )^{n} = \lim_{n \to \infty} \left ( \left ( 1 + \frac{1}{\frac{n}{x}} \right )^{\frac{n}{x}} \right )^{x} \approx e^{x} \] 其中， \[ \lim_{n \to \infty} \left ( 1 + \frac{1}{\frac{n}{x}} \right )^{\frac{n}{x}} \approx e^{1} = e \] 而這是它寫為 \(e^{x}\) 的原因。 wiki-exponential 所以指數函數有基本的恆等式： \[ e^{x+y} = e^{x} \cdot e^{y} \] \[ \parallel \] \[ \exp\left ( x + y \right ) = \exp\left ( x \right ) \cdot \exp\left ( y \right ) \] 性質 所以，正常指數該有的性質 \(e\) 也都有具備，令 \(\forall x, y\in \mathrm{R}\) ，則： \(e^{0}=1\) \(e^{1}=e\) \(e^{x+y}=e^{x}e^{y}\) \(e^{x \cdot y}=\left(e^{x}\right)^{y}\) \(e^{-x}={1 \over e^{x}}\) 微分 微分的時候需要下面這個式子： \[ \lim_{n \to \infty} \left(1 + \frac{1}{n}\right) =\lim_{n \to \infty}\left(\left(1 + \frac{1}{\frac{n}{1}}\right)^{\frac{n}{1}}\right)^{\frac{1}{n}} \approx \lim_{n \to \infty} e^{\frac{1}{n}} \Rightarrow e^{\Delta x} \approx (1 + \Delta x) \] 其中， \(\lim_{n \to \infty} \left ( 1 + \frac{1}{\frac{n}{1}} \right )^{\frac{n}{1}} \approx e^{1} = e\) 當\(n \to \infty\)時 \(\frac{1}{n}\) 可視為一個很小的量 \(\Delta x\) ，也就是： \[ \lim_{m \to 0} (1 + m ) \approx \lim_{m \to 0} e^{m} \] \[ (1 + 很小 ) \approx e^{很小} \] \[ (1 + \Delta x ) \approx e^{\Delta x} \] 微分推導 \[ \Delta y = e^{x+\Delta x} - e^{x} = e^{\Delta x}\cdot e^{x} - 1\cdot e^{x} = (e^{\Delta x} - 1) e^{x} \] 又因： \[ e^{\Delta x} \approx (1 + \Delta x) \] 所以： \[ \Delta y \approx e^{x}\Delta x \rightarrow dy = e^{x}\cdot dx \] 小結： \[ de^{□} = e^{□}d□ \] \(ex.\) \[ de^{-x^2} = e^{-x^2}\cdot d(-x^2) = (-2x)\cdot e^{-x^2}\cdot dx \] 一般化： $y = a^x \(&lt;br&gt;\)y = e^{} = e^{x} $ (對 \(y\) 作微分)\(\Rightarrow dy= e^{x\cdot\ln{a}} \cdot \left ( x\cdot\ln{a} \right )\)\(\Rightarrow dy=a^x\cdot \ln a \cdot dx\) “\(e^{x}\)” 的反函數 對數函數，就是 \(e^{x}\) 的反函數，也就是 \[ y = \log_{e}{x} = \ln{x} \quad x = e^{y} \] img 定義 尤拉定義自然對數為序列的極限： \[ \ln (x) = \lim_{x\rightarrow \infty} n(x^{\frac{1}{n}}- 1) \] 正式的定義為積分\(\ln (a)\)： \[ \ln (a) = \int_1^a \frac{1}{x} dx \] 對 “\(\ln{x}\)” 做微分 簡單地，我們可以推得： $y = x $ $x = e^y $ $dx = e^y dy \(&lt;br&gt;\)dy = dx = dx $ (移&quot; \(e^{y}\) &quot;項) img Proof \(\ln x = \int_1^x \frac{1}{x} dx\) \(\frac{d}{dx}\ln x = \lim_{h \rightarrow 0} (\frac{\ln(x+h) - \ln x}{h})\) \(\Leftrightarrow \lim_{h \rightarrow 0} (\frac{1}{h}\cdot \ln(\frac{x+h}{x}))\) \(\Leftrightarrow \lim_{h \rightarrow 0} (\ln (1+\frac{h}{x})^{\frac{1}{h}})\) ＜Note＞：\(Let \; u = \frac{h}{x} , h = u\cdot x \rightarrow h \; gose \; to \; 0 \; then \; u \; gose \; to \; 0\) \(\Rightarrow \lim_{u \to 0} (\ln (1+u)^{\frac{1}{u\cdot x}} ) \Rightarrow \lim_{u \to 0} (\ln(1+u)^{\frac{1}{u}^\frac{1}{x}})\) \(\Leftrightarrow \lim_{u \to 0} (\frac{1}{x} \cdot ln (1 + u)^{\frac{1}{u}})\) \(\Leftrightarrow \frac{1}{x} \lim_{u \to 0} (\ln(1+u)^{\frac{1}{u}})\) ＜Note＞：$Let ; n = u ; gose ; to ; 0 ; then ; n ; gose ; to ; $ \(\Rightarrow \frac{1}{x} lim_{n \to \infty} (\ln (1 + \frac{1}{n})^n)\) \(\Leftrightarrow \frac{1}{x} \ln( \lim_{n \to \infty} (1 + \frac{1}{n})^n )\) \(\Leftrightarrow \frac{1}{x} \ln e \Leftrightarrow \frac{1}{x}\) $ _1^x dt $（微積分第一基本定理） \[ \frac{d}{dx} \ln x = \frac{d}{dx} \int_1^x \frac{1}{t} dt \Leftrightarrow \ln x = \int_1^x \frac{1}{t} dt \] 一般化： $y=_a x $ \(\Rightarrow y = \frac{\ln{x}}{\ln{a}}\) \(\Rightarrow dy=\frac{1}{x}\cdot \frac{1}{\ln a} \cdot dx\) &quot; \(f \left( x\right) = x^x\) &quot;的微分 兩側取 “\(\ln\)” \[ \ln \left( f \left( x\right) \right) = x \cdot \ln{x} \] 對兩側微分 \[ \frac{f&#39;(x)}{f(x)} = \left( 1 \cdot \ln{x} \right) + \left( x \cdot \frac{1}{x}\right) \] 對兩側乘上 “\(f(x)\)” \[ f&#39;(x) = \left( \left( 1 \cdot \ln{x} \right) + \left( x \cdot \frac{1}{x}\right) \right) \cdot f(x) \] \[ \Rightarrow f&#39;(x) = \left( \ln{x} + 1 \right) \cdot \left( x^x \right) \] 參考 成大微積分指對數函數的微分(第四週共筆) 維基百科 - e (數學常數) 維基百科 - 指數函數 中華科大 - PART 10：指數與對數微分公式彙整]]></content>
      <categories>
        <category>Calculus</category>
      </categories>
      <tags>
        <tag>Exponent</tag>
        <tag>Logarithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MathJax 文法測試]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2FMathJax-test%2F</url>
    <content type="text"><![CDATA[Admin + MathJax + Pandoc 測試 \[ \lim_{n \to \infty}\left ( 1 + \frac{1}{n} \right )^{n} \] \[ \left\{\begin{matrix}a \equiv b (\mod m)\\ c \equiv d (\mod m)\end{matrix}\right. \Rightarrow \left\{\begin{matrix}a \pm c \equiv b \pm d (\mod m)\\ a \cdot c \equiv b \cdot d (\mod m)\end{matrix}\right. \]]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇測試發文]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2F%E7%AC%AC%E4%B8%80%E7%AF%87%E6%B8%AC%E8%A9%A6%E7%99%BC%E6%96%87%2F</url>
    <content type="text"><![CDATA[H1 H2 H3 H4 H5 H6]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>Mark Down</tag>
      </tags>
  </entry>
</search>
